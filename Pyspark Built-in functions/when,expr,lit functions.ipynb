{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b4d8c49-3f8a-4d82-913b-ae85170cf553",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Setting Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa69df5a-5790-4ab5-ba25-4dbecdc1eea4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70d9dcad-91c2-4e1f-a4fd-db16f7b25512",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Create SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b441cfa-46bb-4f34-a87a-4f1915df9f72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"Otherwise-when\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9e4b037-4191-499a-a8db-cdfbf11650a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### when() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bce5e8a5-6545-47cb-ada3-b917c256c965",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n|   name|gender|salary|\n+-------+------+------+\n|  James|     M| 60000|\n|Michael|     M| 70000|\n| Robert|  null|400000|\n|  Maria|     F|500000|\n|    Jen|      |  null|\n+-------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\",\"M\",60000),(\"Michael\",\"M\",70000),\n",
    "        (\"Robert\",None,400000),(\"Maria\",\"F\",500000),\n",
    "        (\"Jen\",\"\",None)]\n",
    "columns = [\"name\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b9d0556-8286-4348-a00d-57f054ed2246",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+\n|   name|gender|salary|new_gender|\n+-------+------+------+----------+\n|  James|     M| 60000|      Male|\n|Michael|     M| 70000|      Male|\n| Robert|  null|400000|       N/A|\n|  Maria|     F|500000|    Female|\n|    Jen|      |  null|       N/A|\n+-------+------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df2=df.withColumn(\"new_gender\",when(df.gender == 'M','Male')\n",
    "                  .when(df.gender == 'F','Female')\n",
    "                  .otherwise('N/A'))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74c93268-9c57-40dd-8e64-10b98ca986c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Usinf select() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60ac8358-c917-417a-bf16-39967039fc9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+\n|   name|gender|salary|new_gender|\n+-------+------+------+----------+\n|  James|     M| 60000|      Male|\n|Michael|     M| 70000|      Male|\n| Robert|  null|400000|       N/A|\n|  Maria|     F|500000|    Female|\n|    Jen|      |  null|       N/A|\n+-------+------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df3=df.select(col(\"*\"),when(df.gender == 'M','Male')\n",
    "              .when(df.gender == 'F','Female')\n",
    "              .otherwise(\"N/A\").alias(\"new_gender\"))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44f4c240-5778-42f5-ac77-1dc071ac52c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Using expr(Case, When) in Sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "312b078f-6b44-4228-b82a-92c369375b3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+\n|   name|gender|salary|new_Gender|\n+-------+------+------+----------+\n|  James|     M| 60000|      Male|\n|Michael|     M| 70000|      Male|\n| Robert|  null|400000|        Na|\n|  Maria|     F|500000|    Female|\n|    Jen|      |  null|        Na|\n+-------+------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "df4=df.withColumn('new_Gender',expr(\n",
    "    \"CASE WHEN gender='M' Then 'Male'\"+\n",
    "          \"WHEN gender='F' Then 'Female'\"+\n",
    "          \"ELSE 'Na' END\"\n",
    "))\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2aa4e16-e623-4857-885e-47f795ebd014",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Using Case on WHEN SQL Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fbba0fd-ae28-41f0-9599-36607a31045f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n|   name|gender|salary|\n+-------+------+------+\n|  James|  Male| 60000|\n|Michael|  Male| 70000|\n| Robert|    NA|400000|\n|  Maria|Female|500000|\n|    Jen|    NA|  null|\n+-------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"sample\")\n",
    "df_sql=spark.sql(\"\"\"\n",
    "                 select name,\n",
    "                 CASE\n",
    "                 WHEN gender='M' THEN 'Male'\n",
    "                 WHEN gender='F' THEN 'Female'\n",
    "                 ELSE 'NA'\n",
    "                 END as gender,\n",
    "                 salary\n",
    "                 FROM sample\n",
    "                 \"\"\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95b5c0f7-6ed7-4fa2-b24a-1481896f6b02",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### expr() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a93ba6e0-4386-40eb-bb64-695c71e7a2c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n| col1| col2|\n+-----+-----+\n|James| Bond|\n|Scott|Varsa|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data=[(\"James\",\"Bond\"),(\"Scott\",\"Varsa\")] \n",
    "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\") \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ccf8604-f26e-4cef-93a0-6a42c806ef1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n|    name|gen|\n+--------+---+\n|   James|  M|\n|Praveena|  F|\n|Yaswanth|   |\n+--------+---+\n\n"
     ]
    }
   ],
   "source": [
    "data1 = [\n",
    "    (\"James\", \"M\"),\n",
    "    (\"Praveena\", \"F\"),\n",
    "    (\"Yaswanth\", \"\")\n",
    "]\n",
    "col1 = [\"name\", \"gen\"]\n",
    "df1 = spark.createDataFrame(data1, col1)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0b552d8-c388-43ff-9ff4-9058fcf79073",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n|      date|increment|\n+----------+---------+\n|2019-01-23|        1|\n|2019-06-24|        2|\n|2019-09-20|        3|\n+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "data2=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)] \n",
    "df2=spark.createDataFrame(data2).toDF(\"date\",\"increment\") \n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55b44238-679a-4c16-8eb7-6a676d8ed593",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Concatenate using expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2b816d2-5ca1-4f6d-8003-1e0fc6c3c615",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------+\n| col1| col2|       Name|\n+-----+-----+-----------+\n|James| Bond| James Bond|\n|Scott|Varsa|Scott Varsa|\n+-----+-----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"Name\",expr(\"col1 ||' '|| col2\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9105e268-6b58-48f9-8a52-ad6e18332b0b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### When using expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cba8fd33-3729-42cc-99b6-e8ef7b8bbada",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+------+\n|    name|gen|gender|\n+--------+---+------+\n|   James|  M|  Male|\n|Praveena|  F|Female|\n|Yaswanth|   |    NA|\n+--------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn(\"gender\",expr(\n",
    "    \"\"\"\n",
    "    CASE\n",
    "    WHEN gen=\"M\" THEN \"Male\"\n",
    "    WHEN gen=\"F\" THEN \"Female\"\n",
    "    ELSE 'NA'\n",
    "    END\n",
    "    \"\"\"\n",
    ")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77e0f9cc-de0d-41be-89ce-68d3861f25eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Using Exiting column vlue for expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03ca3530-fab6-48ed-aa5f-f7e9e8cfcdf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------------+\n|      date|increment|updated_timestamp|\n+----------+---------+-----------------+\n|2019-01-23|        1|       2019-02-23|\n|2019-06-24|        2|       2019-08-24|\n|2019-09-20|        3|       2019-12-20|\n+----------+---------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df2.withColumn(\"updated_timestamp\",expr('add_months(date,increment)')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24a77596-f5f0-4a9a-ad86-99748859e5bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Using alias with the column expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3de9e4c6-11c1-481d-9f32-f147a5231f89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+\n|      date|increment|  new_date|\n+----------+---------+----------+\n|2019-01-23|        1|2019-02-23|\n|2019-06-24|        2|2019-08-24|\n|2019-09-20|        3|2019-12-20|\n+----------+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df2.select(df2.date,df2.increment,expr('add_months(date,increment) as new_date')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2d5e77d-53b0-4d49-9646-c32d9bbb5b6f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### cast function with expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "506f4c66-8667-434c-8390-01fe5108789b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- increment: long (nullable = true)\n |-- increment_str: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df2.select(\"increment\",expr(\"cast(increment as string) as increment_str\"))\\\n",
    "    .printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e7b0bf0-0f1b-475d-8194-a6fa073a3f4f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### filter function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07736da0-99af-4b0e-8a41-7295047982a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n|col1|col2|\n+----+----+\n| 100|   2|\n| 200|3000|\n| 500| 500|\n+----+----+\n\n+----+----+\n|col1|col2|\n+----+----+\n| 100|   2|\n+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "data3=[(100,2),(200,3000),(500,500)]\n",
    "df3=spark.createDataFrame(data3,['col1','col2'])\n",
    "df3.show()\n",
    "df3.filter(expr('col1>col2')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c697be32-20a6-4b2f-b54e-8648de809188",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### lit() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2124e374-7fd2-42c5-af57-db6ed1d14c32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n|EmpId|Salary|\n+-----+------+\n|  111| 50000|\n|  222| 60000|\n|  333| 40000|\n+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(\"111\",50000),(\"222\",60000),(\"333\",40000)]\n",
    "df = spark.createDataFrame(data, [\"EmpId\", \"Salary\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0d739c8-41ab-4191-9695-913f4929f18d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### lit() with select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33b56e3e-95fd-4d58-a17e-8f3c6c9d900c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+\n|EmpId|Salary|lit|\n+-----+------+---+\n|  111| 50000|  1|\n|  222| 60000|  1|\n|  333| 40000|  1|\n+-----+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.select(\"EmpId\",\"Salary\",lit(1).alias(\"lit\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4185c7b-a8fd-4f98-b6f4-cdb2dcbf8b10",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### lit() with withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ec06bc2-3453-497d-b207-35abb8bcffa8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------+\n|EmpId|Salary|new_col|\n+-----+------+-------+\n|  111| 50000|    200|\n|  222| 60000|    200|\n|  333| 40000|    100|\n+-----+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"new_col\",when(df.Salary>40000,lit(200)).otherwise(lit(100))).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "when,expr,lit functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
