{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39fbff71-dff3-4bb2-a693-73932a648b56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e113c6af-face-49ea-99a3-07123bf1e9ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"built-in\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab8ff430-aa74-4e7e-9888-b1ae9d6fb81e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Creating data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "888dc1cd-9f25-4f35-9495-bf23aef13eb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: string (nullable = true)\n |-- dob_year: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = [(\"James, A, Smith\",\"2018\",\"M\",3000),\n",
    "            (\"Michael, Rose, Jones\",\"2010\",\"M\",4000),\n",
    "            (\"Robert,K,Williams\",\"2010\",\"M\",4000),\n",
    "            (\"Maria,Anne,Jones\",\"2005\",\"F\",4000),\n",
    "            (\"Jen,Mary,Brown\",\"2010\",\"\",-1)\n",
    "            ]\n",
    "\n",
    "columns=[\"name\",\"dob_year\",\"gender\",\"salary\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "535d8d6f-e952-4aaf-9518-2657ab401ad1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------+------+\n|                name|dob_year|gender|salary|\n+--------------------+--------+------+------+\n|     James, A, Smith|    2018|     M|  3000|\n|Michael, Rose, Jones|    2010|     M|  4000|\n|   Robert,K,Williams|    2010|     M|  4000|\n|    Maria,Anne,Jones|    2005|     F|  4000|\n|      Jen,Mary,Brown|    2010|      |    -1|\n+--------------------+--------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce48ace2-82b9-42d4-b6ff-0e3871bbdfc9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Converting String to Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "354f6254-fd5f-4bad-a70f-c9bfc18eb869",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n|full_name               |\n+------------------------+\n|[James,  A,  Smith]     |\n|[Michael,  Rose,  Jones]|\n|[Robert, K, Williams]   |\n|[Maria, Anne, Jones]    |\n|[Jen, Mary, Brown]      |\n+------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "# split used to convert any string into array/list\n",
    "df.select(split(df.name,\",\").alias(\"full_name\"))\\\n",
    "    .drop(\"name\").show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8854dd32-82d2-49fb-b781-a586be281682",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### converting String to Array(Using sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddc66ec8-ed46-4a0b-ad3a-8ee271c2f634",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n|                name|           full_name|\n+--------------------+--------------------+\n|     James, A, Smith| [James,  A,  Smith]|\n|Michael, Rose, Jones|[Michael,  Rose, ...|\n|   Robert,K,Williams|[Robert, K, Willi...|\n|    Maria,Anne,Jones|[Maria, Anne, Jones]|\n|      Jen,Mary,Brown|  [Jen, Mary, Brown]|\n+--------------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"employee\")\n",
    "spark.sql(\"\"\"\n",
    "         select name,SPLIT(name,\",\") as full_name from employee\n",
    "         \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0ebcd7b-511f-4564-bc8f-15a7213090ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### concat_ws() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c728343-fef7-4201-a7cb-ed34d340eb9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: string (nullable = true)\n |-- languagesAtSchool: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- currentState: string (nullable = true)\n\n+----------------+------------------+------------+\n|            name| languagesAtSchool|currentState|\n+----------------+------------------+------------+\n|    James,,Smith|[Java, Scala, C++]|          CA|\n|   Michael,Rose,|[Spark, Java, C++]|          NJ|\n|Robert,,Williams|      [CSharp, VB]|          NV|\n+----------------+------------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \n",
    "    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
    "    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5267ce51-1553-45c4-b1aa-85aa42c06bff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Combining array elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1cb23ff-bf57-48a9-bb9f-4deb14138bdd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+\n| languagesAtSchool|       new_col|\n+------------------+--------------+\n|[Java, Scala, C++]|Java;Scala;C++|\n|[Spark, Java, C++]|Spark;Java;C++|\n|      [CSharp, VB]|     CSharp;VB|\n+------------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "df.select(df.languagesAtSchool)\\\n",
    "    .withColumn(\"new_col\",concat_ws(\";\",df.languagesAtSchool)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b989e575-5a7a-4d68-8708-4fd4710d6474",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### combining array elements(using sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1db12bc7-8c40-4c9c-a463-37b2a36b35de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+\n| languagesAtSchool|       new_col|\n+------------------+--------------+\n|[Java, Scala, C++]|Java;Scala;C++|\n|[Spark, Java, C++]|Spark;Java;C++|\n|      [CSharp, VB]|     CSharp;VB|\n+------------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"school\")\n",
    "spark.sql(\"\"\"\n",
    "          select languagesAtSchool,concat_ws(\";\",languagesAtSchool) as new_col from school\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5883fed0-f3c8-4ce2-b7e2-d697153ef7f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### substring() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74f56023-5087-4e2f-9076-d243f715efb7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- calender: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "data=[(1,\"20200828\"),(2,\"20180525\")]\n",
    "columns=['id','calender']\n",
    "df=spark.createDataFrame(data,columns)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3079baf8-21e7-4d45-8b14-cc7134dbd025",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n| id|calender|\n+---+--------+\n|  1|20200828|\n|  2|20180525|\n+---+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2818ddb6-ed10-4d6d-91be-c250aa550cde",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### withColumn using substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fd5ba8c-2dbf-4578-8669-7e4d5ca7e215",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+-----+----+\n| id|calender|Year|Month|date|\n+---+--------+----+-----+----+\n|  1|20200828|2020|   08|  28|\n|  2|20180525|2018|   05|  25|\n+---+--------+----+-----+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import substring\n",
    "df.withColumn(\"Year\",substring(\"calender\",1,4))\\\n",
    "    .withColumn(\"Month\",substring(\"calender\",5,2))\\\n",
    "        .withColumn(\"date\",substring(\"calender\",7,2))\\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6f081b0-ac21-42ee-a4fb-73a3d3b41914",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### select using substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f32656af-3606-4875-8cc5-4680a96cfecc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----+----+\n|calender|Year|Month|Date|\n+--------+----+-----+----+\n|20200828|2020|   08|  28|\n|20180525|2018|   05|  25|\n+--------+----+-----+----+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(\"calender\",substring(df.calender,1,4).alias(\"Year\"),\n",
    "          substring(df.calender,5,2).alias(\"Month\"),\n",
    "          substring(df.calender,7,2).alias(\"Date\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3166a48c-a10c-48d4-bc06-432ffae046d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### with selectExpr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db16d365-d53b-45a0-85cd-9d7010c999f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----+----+\n|calender|year|month|date|\n+--------+----+-----+----+\n|20200828|2020|   08|  28|\n|20180525|2018|   05|  25|\n+--------+----+-----+----+\n\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr('calender','substring(calender,1,4) as year',\n",
    "              'substring(calender,5,2) as month',\n",
    "              'substring(calender,7,2) as date')\\\n",
    "                  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd794592-e0b5-4ee4-b2d5-f2d95942d071",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### substr() from column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be5b3384-b725-4d2d-b3e3-3f980f8b3e78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+-----+----+\n| id|calender|year|month|date|\n+---+--------+----+-----+----+\n|  1|20200828|2020|   08|  28|\n|  2|20180525|2018|   05|  25|\n+---+--------+----+-----+----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.withColumn(\"year\", df.calender.substr(1, 4))\\\n",
    "    .withColumn(\"month\", df.calender.substr(5, 2))\\\n",
    "    .withColumn(\"date\", df.calender.substr(7, 2))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66adc275-dc8e-41f6-b69c-a39342d80dbd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Using sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f9b8320-d6a8-4752-bcc6-5809ca7213de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+-----+----+\n| id|calender|year|month|date|\n+---+--------+----+-----+----+\n|  1|20200828|2020|   08|  28|\n|  2|20180525|2018|   05|  25|\n+---+--------+----+-----+----+\n\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"details\")\n",
    "spark.sql(\"\"\"\n",
    "          SELECT id,calender,\n",
    "          substring(calender,1,4) as year,\n",
    "          substring(calender,5,2) as month,\n",
    "          substring(calender,7,2) as date from details\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b90a8143-e7fe-4bd1-a0b5-86e363b07f44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+-----+----+\n| id|calender|year|month|date|\n+---+--------+----+-----+----+\n|  1|20200828|2020|   08|  28|\n|  2|20180525|2018|   05|  25|\n+---+--------+----+-----+----+\n\n"
     ]
    }
   ],
   "source": [
    "select_statement='Select id,calender, substring(calender,1,4) as year, substring(calender,5,2) as month, substring(calender,7,2) as date from details'\n",
    "spark.sql(f\"{select_statement}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f1a5d28-daee-40ac-9b7b-762491d4aca2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----+\n| id|           address|state|\n+---+------------------+-----+\n|  1|  14851 Jeffrey Rd|   DE|\n|  2|43421 Margarita St|   NY|\n|  3|  13111 Siemon Ave|   CA|\n+---+------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "address = [(1,\"14851 Jeffrey Rd\",\"DE\"),\n",
    "    (2,\"43421 Margarita St\",\"NY\"),\n",
    "    (3,\"13111 Siemon Ave\",\"CA\")]\n",
    "df=spark.createDataFrame(address,['id','address','state'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b73bfc9-b481-4e46-be4b-fa2a71b214f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Replace String with columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3d3c761-3387-4320-8b87-60b7a3773570",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------------------+\n| id|state|          new_addr|\n+---+-----+------------------+\n|  1|   DE|14851 Jeffrey Road|\n|  2|   NY|43421 Margarita St|\n|  3|   CA|  13111 Siemon Ave|\n+---+-----+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "df.withColumn(\"new_addr\",regexp_replace('address','Rd','Road'))\\\n",
    "    .drop(\"address\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2e61ba2-3c4b-4bd9-ac08-e9c9181cd7ee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Replce Column values conditionally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "574e9fe4-14d4-43ef-a3e3-fb6f98ab8e54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+\n| id|             address|state|\n+---+--------------------+-----+\n|  1|  14851 Jeffrey Road|   DE|\n|  2|43421 Margarita S...|   NY|\n|  3| 13111 Siemon Avenue|   CA|\n+---+--------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df.withColumn('address',when(df.address.endswith(\"Rd\"),regexp_replace(df.address,'Rd','Road'))\\\n",
    "    .when(df.address.endswith(\"St\"),regexp_replace(df.address,'St','Street'))\\\n",
    "             .when(df.address.endswith(\"Ave\"), regexp_replace(df.address, 'Ave', 'Avenue'))\\\n",
    "            .otherwise(df.address))\\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a760f57e-ca6b-46aa-9f7e-4ebd7ee18a52",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Replace column with other column value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3863d5c-4c6a-455d-9ac0-ca885f1073de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df3 = spark.createDataFrame(\n",
    "   [(\"ABCDE_XYZ\", \"XYZ\",\"FGH\")], \n",
    "    (\"col1\", \"col2\",\"col3\")\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "512d2874-1ca0-430e-a605-7e15efe81ea0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----+---------+\n|     col1|col2|col3|  new_col|\n+---------+----+----+---------+\n|ABCDE_XYZ| XYZ| FGH|ABCDE_FGH|\n+---------+----+----+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "# col1 is the column that has to be replaced with wherever the values from the col2 should be replaced with col3\n",
    "df3.withColumn(\"new_col\",\n",
    "              expr(\"regexp_replace(col1, col2, col3)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7b635b3-1aa7-4a4d-b273-e01e4aaca825",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Replace column values with dictionry values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a52685e5-e235-4a60-bbd6-e52b7b19864c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+----------+\n| id|           address|     state|\n+---+------------------+----------+\n|  1|  14851 Jeffrey Rd|  Delaware|\n|  2|43421 Margarita St|   NewYork|\n|  3|  13111 Siemon Ave|California|\n+---+------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "stateDict = {\"DE\":\"Delaware\", \"NY\":\"NewYork\", \"CA\":\"California\"}\n",
    "df2 = df.rdd.map(lambda x :\n",
    "                (x.id, x.address, stateDict[x.state])\n",
    "                ).toDF([\"id\", \"address\", \"state\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9fe3c5e-f3f7-48bd-a9c4-23978da65254",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### translate() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0473bdeb-1e05-408a-9723-3cdb441e05bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----+------------------+\n| id|           address|state|           new_add|\n+---+------------------+-----+------------------+\n|  1|  14851 Jeffrey Rd|   DE|  A485A Jeffrey Rd|\n|  2|43421 Margarita St|   NY|4C4BA Margarita St|\n|  3|  13111 Siemon Ave|   CA|  ACAAA Siemon Ave|\n+---+------------------+-----+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import translate\n",
    "df.withColumn(\"new_add\", translate('address','123','ABC')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf4b345f-57e0-4a59-a912-8d20642b818a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### overlay() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "066fcc2c-ceb3-481f-9c29-d878d88812a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- col1: string (nullable = true)\n |-- col2: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df4 = spark.createDataFrame([(\"ABCDE_XYZ\", \"FGH\")], (\"col1\", \"col2\"))\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5987393-6ab9-4c6d-a672-a1330f540c07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-----------+\n|     col1|col2|new_overlay|\n+---------+----+-----------+\n|ABCDE_XYZ| FGH|ABCDE_XYFGH|\n+---------+----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import overlay\n",
    "df4.withColumn(\"new_overlay\", overlay(\"col1\", \"col2\", 9)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abacdcf8-c0b9-40b8-9611-4a5249df89f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Built-in functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
