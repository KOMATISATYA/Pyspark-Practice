{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab5d1ad6-4e31-4c25-b102-3025e3795204",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Setting Environmental Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d1b6516-0843-43d0-b0fc-84903681c06a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc893616-a600-4ef6-8106-f3675e5e8e21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Creating SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0db7ca8d-9a15-4b0b-a4ef-9f1add9605a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"Explode\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "790086dd-6ab6-4fe4-9fb3-1ae56edec8a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4515f130-26f1-44ca-897e-fc27a6470df8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: string (nullable = true)\n |-- subjects: array (nullable = true)\n |    |-- element: array (containsNull = true)\n |    |    |-- element: string (containsNull = true)\n\n+-------+--------------------+\n|   name|            subjects|\n+-------+--------------------+\n|  James|[[Java, Scala, C+...|\n|Michael|[[Spark, Java, C+...|\n| Robert|[[CSharp, VB], [S...|\n+-------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "ArrayData = [\n",
    "  (\"James\",[[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]]),\n",
    "  (\"Michael\",[[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]]),\n",
    "  (\"Robert\",[[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"]])\n",
    "]\n",
    "df=spark.createDataFrame(data=ArrayData,schema=['name','subjects'])\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69debfb5-15fa-4298-a82c-4f8c2587e501",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n|   name|               col|\n+-------+------------------+\n|  James|[Java, Scala, C++]|\n|  James|     [Spark, Java]|\n|Michael|[Spark, Java, C++]|\n|Michael|     [Spark, Java]|\n| Robert|      [CSharp, VB]|\n| Robert|   [Spark, Python]|\n+-------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df.select(df.name,explode(df.subjects))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58da7440-9af6-4c8e-b9cc-595b4570682f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "353d5e07-711c-4df0-bd7c-5caa3f27e734",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n|   name|                subs|\n+-------+--------------------+\n|  James|[Java, Scala, C++...|\n|Michael|[Spark, Java, C++...|\n| Robert|[CSharp, VB, Spar...|\n+-------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import flatten\n",
    "df.select(df.name,flatten(df.subjects).alias(\"subs\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ccb6968-9417-4493-9760-9dfe99207dd1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "251a3bb6-4f9f-4c06-b059-e206002a9472",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+---------------+------------+-------------+\n|name            |languagesAtSchool |languagesAtWork|currentState|previousState|\n+----------------+------------------+---------------+------------+-------------+\n|James,,Smith    |[Java, Scala, C++]|[Spark, Java]  |OH          |CA           |\n|Michael,Rose,   |[Spark, Java, C++]|[Spark, Java]  |NY          |NJ           |\n|Robert,,Williams|[CSharp, VB]      |[Spark, Python]|UT          |NV           |\n+----------------+------------------+---------------+------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, ArrayType,StructType,StructField\n",
    "data = [\n",
    " (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"],\"OH\",\"CA\"),\n",
    " (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"],\"NY\",\"NJ\"),\n",
    " (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"],\"UT\",\"NV\")\n",
    "]\n",
    "schema = StructType([ \n",
    "    StructField(\"name\",StringType(),True), \n",
    "    StructField(\"languagesAtSchool\",ArrayType(StringType()),True), \n",
    "    StructField(\"languagesAtWork\",ArrayType(StringType()),True), \n",
    "    StructField(\"currentState\", StringType(), True), \n",
    "    StructField(\"previousState\", StringType(), True)\n",
    "  ])\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ce8c1af-8bcd-466e-8088-eb1e8c46cf4c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### explode example in array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bf864d1-9deb-4784-8071-6a1c34a38e48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n|            name|   col|\n+----------------+------+\n|    James,,Smith|  Java|\n|    James,,Smith| Scala|\n|    James,,Smith|   C++|\n|   Michael,Rose,| Spark|\n|   Michael,Rose,|  Java|\n|   Michael,Rose,|   C++|\n|Robert,,Williams|CSharp|\n|Robert,,Williams|    VB|\n+----------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df.select(df.name,explode(df.languagesAtSchool)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be1190a7-5f8c-4b5b-b707-1857e0c3a7ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c8b9905-f615-46f1-94f0-42aa0483dd61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n|          split_name|languagesAtWork|\n+--------------------+---------------+\n|    [James, , Smith]|  [Spark, Java]|\n|   [Michael, Rose, ]|  [Spark, Java]|\n|[Robert, , Williams]|[Spark, Python]|\n+--------------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "df.select(split(df.name,',').alias(\"split_name\"),df.languagesAtWork).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83de894e-ac1e-4e40-9e5d-170396e6f3df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b703797-3216-4184-abe3-1c495505d1fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+\n|            name|   State|\n+----------------+--------+\n|    James,,Smith|[OH, CA]|\n|   Michael,Rose,|[NY, NJ]|\n|Robert,,Williams|[UT, NV]|\n+----------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array\n",
    "df.select(df.name,array(df.currentState,df.previousState).alias('State')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fc68cc7-7df5-4473-b5b9-8146c84da36d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### array_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "835786c7-eaa9-43e3-83fe-b03a3b940365",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Enter a language to check if it exists : Java"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+\n|            name|isExists|\n+----------------+--------+\n|    James,,Smith|    true|\n|   Michael,Rose,|    true|\n|Robert,,Williams|   false|\n+----------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "row=input(\"Enter a language to check if it exists :\")\n",
    "df.select(df.name,array_contains(df.languagesAtSchool,row).alias(\"isExists\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e88f1ae7-edef-46c7-a6e8-0770912696ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### collect_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1191727-610c-466a-80e4-dbb12fd7385a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+------------+-------------+---------+\n|            name|languagesAtWork|currentState|previousState|languages|\n+----------------+---------------+------------+-------------+---------+\n|    James,,Smith|  [Spark, Java]|          OH|           CA|     Java|\n|    James,,Smith|  [Spark, Java]|          OH|           CA|    Scala|\n|    James,,Smith|  [Spark, Java]|          OH|           CA|      C++|\n|   Michael,Rose,|  [Spark, Java]|          NY|           NJ|    Spark|\n|   Michael,Rose,|  [Spark, Java]|          NY|           NJ|     Java|\n|   Michael,Rose,|  [Spark, Java]|          NY|           NJ|      C++|\n|Robert,,Williams|[Spark, Python]|          UT|           NV|   CSharp|\n|Robert,,Williams|[Spark, Python]|          UT|           NV|       VB|\n+----------------+---------------+------------+-------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn(\"languages\",explode(\"languagesAtSchool\")).drop(\"languagesAtSchool\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5482cdbe-6841-4d6e-9b79-93a8033f8916",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n|collect_list(languages)|\n+-----------------------+\n|   [Java, Scala, C++...|\n+-----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "df.select(collect_list(\"languages\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61db4390-bdcd-4874-9d7d-c451f0cdaf39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### collect_set() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7703e5e8-a9e7-40d4-b81c-01c24c5c787f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n|collect_set(languages)|\n+----------------------+\n|  [CSharp, VB, Scal...|\n+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set\n",
    "df.select(collect_set(\"languages\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce023d52-bf15-4ed7-a463-5df9b0501f40",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### count() and countDistinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a5b5356-6953-47af-b357-57687660e7b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n|count(languages)|\n+----------------+\n|               8|\n+----------------+\n\n+-------------------------+\n|count(DISTINCT languages)|\n+-------------------------+\n|                        6|\n+-------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count,countDistinct\n",
    "df.select(count(\"languages\")).show()\n",
    "df.select(countDistinct(\"languages\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31fe1837-4fe8-4d40-935f-3ed73db7436c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Map Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dab923c5-f052-4942-b367-3f715ab8538e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### create_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b36657b-4eef-41f5-9c1e-460aa71cef71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: string (nullable = true)\n |-- dept: string (nullable = true)\n |-- salary: integer (nullable = true)\n |-- location: string (nullable = true)\n\n+-----+---------+------+--------+\n|id   |dept     |salary|location|\n+-----+---------+------+--------+\n|36636|Finance  |3000  |USA     |\n|40288|Finance  |5000  |IND     |\n|42114|Sales    |3900  |USA     |\n|39192|Marketing|2500  |CAN     |\n|34534|Sales    |6500  |USA     |\n+-----+---------+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType\n",
    "data = [ (\"36636\",\"Finance\",3000,\"USA\"), \n",
    "    (\"40288\",\"Finance\",5000,\"IND\"), \n",
    "    (\"42114\",\"Sales\",3900,\"USA\"), \n",
    "    (\"39192\",\"Marketing\",2500,\"CAN\"), \n",
    "    (\"34534\",\"Sales\",6500,\"USA\") ]\n",
    "schema = StructType([\n",
    "     StructField('id', StringType(), True),\n",
    "     StructField('dept', StringType(), True),\n",
    "     StructField('salary', IntegerType(), True),\n",
    "     StructField('location', StringType(), True)\n",
    "     ])\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a715305-0909-45b5-86b0-7aa53242786e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### convert dataframe columns to map type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d9fdb08-c0fc-4858-b505-a2bba073cc35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------------------------------+\n|id   |dept     |propertiesMap                    |\n+-----+---------+---------------------------------+\n|36636|Finance  |{salary -> 3000, location -> USA}|\n|40288|Finance  |{salary -> 5000, location -> IND}|\n|42114|Sales    |{salary -> 3900, location -> USA}|\n|39192|Marketing|{salary -> 2500, location -> CAN}|\n|34534|Sales    |{salary -> 6500, location -> USA}|\n+-----+---------+---------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map,lit,col\n",
    "df.withColumn(\"propertiesMap\",create_map(\n",
    "    lit(\"salary\"),col(\"salary\"),\n",
    "    lit(\"location\"),col(\"location\")\n",
    "))\\\n",
    "    .drop(\"salary\",\"location\")\\\n",
    "        .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52e1ad29-174e-4ad7-9ff2-3b5869e83edd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Exploding Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17b8563f-5ea1-42d2-b6bb-b82b2eb78af6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------------+\n|name      |properties                   |\n+----------+-----------------------------+\n|James     |{eye -> brown, hair -> black}|\n|Michael   |{eye -> null, hair -> brown} |\n|Robert    |{eye -> black, hair -> red}  |\n|Washington|{eye -> grey, hair -> grey}  |\n|Jefferson |{eye -> , hair -> brown}     |\n+----------+-----------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import MapType\n",
    "schema = StructType([\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('properties', MapType(StringType(),StringType()),True)\n",
    "])\n",
    "dataDictionary = [\n",
    "        ('James',{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',{'hair':'brown','eye':None}),\n",
    "        ('Robert',{'hair':'red','eye':'black'}),\n",
    "        ('Washington',{'hair':'grey','eye':'grey'}),\n",
    "        ('Jefferson',{'hair':'brown','eye':''})\n",
    "        ]\n",
    "df = spark.createDataFrame(data=dataDictionary, schema = schema)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdcbb289-9d4b-4b10-9e59-c74df32cc711",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df2=df.rdd.map(lambda x:\\\n",
    "    (x.name,x.properties['hair'],x.properties['eye']))\\\n",
    "        .toDF(['name','hair','eye'])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc605b1d-03fe-401c-aba7-5267bd0e3f30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+\n|      name| key|value|\n+----------+----+-----+\n|     James| eye|brown|\n|     James|hair|black|\n|   Michael| eye| null|\n|   Michael|hair|brown|\n|    Robert| eye|black|\n|    Robert|hair|  red|\n|Washington| eye| grey|\n|Washington|hair| grey|\n| Jefferson| eye|     |\n| Jefferson|hair|brown|\n+----------+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(df.name,explode(df.properties)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "164962d2-5577-453d-b212-9f42649c520a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### map_key() functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed722437-276b-4947-9626-189f8aa1f82b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n|      name|       keys|\n+----------+-----------+\n|     James|[eye, hair]|\n|   Michael|[eye, hair]|\n|    Robert|[eye, hair]|\n|Washington|[eye, hair]|\n| Jefferson|[eye, hair]|\n+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_keys\n",
    "df.select(df.name,map_keys(df.properties).alias(\"keys\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "143d5ba6-1784-4b9d-999a-44302bf02e1c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### map_values() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d9c3852-0e15-47ac-b1b4-5b65ddb141b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------+\n|      name|map_values(properties)|\n+----------+----------------------+\n|     James|        [brown, black]|\n|   Michael|         [null, brown]|\n|    Robert|          [black, red]|\n|Washington|          [grey, grey]|\n| Jefferson|             [, brown]|\n+----------+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_values\n",
    "df.select(df.name,map_values(df.properties)).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "explode,collect,map functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
