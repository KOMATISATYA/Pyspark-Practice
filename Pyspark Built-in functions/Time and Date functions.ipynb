{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3d3417c-0e3e-46e8-b41a-1039856bf134",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Setting environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9816a6c1-3a87-41f1-bff9-8277b52ea305",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a9df924-ece2-4e93-84c5-753bdd60045b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb34377d-e383-40c0-8c71-abf0375152a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"DateAndTime\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5c36208-ea72-4893-a192-05b5e98f2090",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: string (nullable = true)\n |-- input_timestamp: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame(\n",
    "    data=[(\"1\",\"2019-06-24 12:01:19.000\")],\n",
    "    schema=[\"id\",\"input_timestamp\"]\n",
    ")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93b3a748-f3ac-4d61-8d4f-323d4ea353e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Casting to TimeStamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61b96ba5-7808-4401-83d4-dc9e4981ecdf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### to_timestamp() for casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7511e6d9-fd29-4227-9add-7b65a88d313f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: string (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "df.select(\"id\",to_timestamp(\"input_timestamp\").alias(\"timestamp\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1431afa5-d6e6-4589-99b2-40c3a5c95c71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: string (nullable = true)\n |-- input_timestamp: string (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n\n+---+--------------------+-------------------+\n| id|     input_timestamp|          timestamp|\n+---+--------------------+-------------------+\n|  1|2019-06-24 12:01:...|2019-06-24 12:01:19|\n+---+--------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df2=df.withColumn(\"timestamp\",to_timestamp(\"input_timestamp\"))\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "352a13c7-1580-48d4-ac6c-405a06f4663f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: string (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n |-- timestamp: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df2.select(col(\"id\"),col(\"timestamp\"),col(\"timestamp\").cast(\"string\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "392ccf19-b0fc-4cf4-86ad-2d0f90f11d59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------------+-------------------+\n| id|     input_timestamp|          timestamp|            current|\n+---+--------------------+-------------------+-------------------+\n|  1|2019-06-24 12:01:...|2019-06-24 12:01:19|09-03-2024 06:48:34|\n+---+--------------------+-------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp,date_format\n",
    "df2.withColumn(\"current\",date_format(current_timestamp(),\"dd-MM-yyyy HH:mm:ss\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "065a3c72-67a7-485b-b220-66b76aad3132",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Using SQL Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2f8deca-bee5-463c-8fa5-096bd5abafdf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+\n| id|         time_stamp|        create_time|\n+---+-------------------+-------------------+\n|  1|2019-06-24 12:01:19|09/03/2024 07:27:30|\n+---+-------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"sample_time\")\n",
    "spark.sql(\"select int(id), to_timestamp(input_timestamp) as time_stamp, date_format(current_timestamp(), 'dd/MM/yyyy HH:mm:ss') as create_time from sample_time\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1a79441-cabf-42c7-bbae-a2c2b02884d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n|          timestamp|\n+-------------------+\n|2019-06-24 12:01:19|\n+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select to_timestamp('06-24-2019 12:01:19.000','MM-dd-yyyy HH:mm:ss.SSSS') as timestamp\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdf466e6-9646-49ee-9590-d704de88c056",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### to_date() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9153eb02-24e9-4fd1-b2cd-c451b921524a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: string (nullable = true)\n |-- input_timestamp: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df=spark.createDataFrame(\n",
    "        data = [ (\"1\",\"2019-06-24 12:01:19.000\")],\n",
    "        schema=[\"id\",\"input_timestamp\"])\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "896ad203-b6d8-454d-ab56-c4ecdbb3033a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+\n| id|     input_timestamp|      date|\n+---+--------------------+----------+\n|  1|2019-06-24 12:01:...|2019-06-24|\n+---+--------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "df.withColumn(\"date\",to_date('input_timestamp')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5f6ea5d-f59f-42c8-a92e-f2d4fc85d56c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+\n| id|     input_timestamp|      date|\n+---+--------------------+----------+\n|  1|2019-06-24 12:01:...|2019-06-24|\n+---+--------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"date\", to_date(\"input_timestamp\",'yyyy-MM-dd HH:mm:ss.SSSS')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03fe7a3f-8b75-4d05-8638-0ee129c5ca7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------------+----------+\n| id|     input_timestamp|                 ts|      date|\n+---+--------------------+-------------------+----------+\n|  1|2019-06-24 12:01:...|2019-06-24 12:01:19|2019-06-24|\n+---+--------------------+-------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.withColumn(\"ts\",to_timestamp(col(\"input_timestamp\")))\\\n",
    "    .withColumn(\"date\",to_date(col(\"ts\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d7c8c66-b99a-454c-9734-8f20208bc102",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+\n| id|     input_timestamp|      date|\n+---+--------------------+----------+\n|  1|2019-06-24 12:01:...|2019-06-24|\n+---+--------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"date\",col(\"input_timestamp\").cast(\"date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2285cce-7ba7-4439-b1f4-79893910118b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: string (nullable = true)\n |-- input_timestamp: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "295787a6-2650-4d49-b2a0-1215fbeb2481",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+\n| id|     input_timestamp|      date|\n+---+--------------------+----------+\n|  1|2019-06-24 12:01:...|2019-06-24|\n+---+--------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('date', to_timestamp('input_timestamp').cast('date'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f708f89-4420-447f-bcc4-3f6016380c42",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### spark sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04f5f717-2a57-43c4-bfff-cfb5e0d57510",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|      date|\n+----------+\n|2019-06-24|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select date('2019-06-24 12:01:19.000') as date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04af2af1-41ab-472c-b99d-3faeaca4194e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n| date_type|\n+----------+\n|2019-06-24|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select date(to_timestamp('2019-06-24 12:01:19.000')) as date_type\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fd7b4a3-f101-47f8-a5db-2812e3ab8562",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n| date_type|\n+----------+\n|2019-06-24|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select to_date('06-24-2019 12:01:19.000','MM-dd-yyyy HH:mm:ss.SSSS') as date_type\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d930866f-e003-4931-8a77-fa9ea97b4632",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### date_format() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "083145be-c7c8-4d85-8c7f-94d01d3c64db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.createDataFrame([[\"1\"]],[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "087d40b8-aef6-468a-a29f-cc7917bf77cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### various formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "726c7806-1f59-4f89-96bf-0c24cf85932e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------------+-----------+-------------+----------------------+--------------+\n|current_date|yyyy-MM-dd|yyyy/MM/dd HH:mm|yyyy MMM dd|dd MMMM yyyy |am/pm                 |HH:mm day     |\n+------------+----------+----------------+-----------+-------------+----------------------+--------------+\n|2024-03-09  |2024-03-09|2024/03/09 07:51|2024 Mar 09|09 March 2024|09-03-2024 07:51:56 AM|07:51 Saturday|\n+------------+----------+----------------+-----------+-------------+----------------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date\n",
    "df.select(current_date().alias(\"current_date\"),\n",
    "         date_format(current_timestamp(), \"yyyy-MM-dd\").alias(\"yyyy-MM-dd\"),\n",
    "         date_format(current_timestamp(), \"yyyy/MM/dd HH:mm\").alias(\"yyyy/MM/dd HH:mm\"),\n",
    "         date_format(current_timestamp(), \"yyyy MMM dd\").alias(\"yyyy MMM dd\"),\n",
    "         date_format(current_timestamp(), \"dd MMMM yyyy\").alias(\"dd MMMM yyyy\"),\n",
    "         date_format(current_timestamp(), \"dd-MM-yyyy hh:mm:ss a\").alias(\"am/pm\"),\n",
    "         date_format(current_timestamp(), \"HH:mm EEEE\").alias(\"HH:mm day\"))\\\n",
    "        .show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbcda667-bb24-4730-ba95-27fee742c1ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### datediff() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f703816-54df-4d0d-8aea-5a909bb12bcf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"1\",\"2019-07-01\"),(\"2\",\"2019-06-24\"),(\"3\",\"2019-08-24\")]\n",
    "df=spark.createDataFrame(data=data,schema=[\"id\",\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51b891cc-a683-4b8e-a65b-6d36fb2fbd82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------------------------+\n|      date|     today|datediff(current_date(), date)|\n+----------+----------+------------------------------+\n|2019-07-01|2024-03-09|                          1713|\n|2019-06-24|2024-03-09|                          1720|\n|2019-08-24|2024-03-09|                          1659|\n+----------+----------+------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff\n",
    "df.select(col(\"date\"),\n",
    "         current_date().alias(\"today\"),\n",
    "         datediff(current_date(), col(\"date\")))\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d20bd94-2296-4b4e-b988-a0cf9819924b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### months_between() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "495eadd1-8afe-4bf5-9604-c354f05408b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------+----------------------+-------------+\n| id|      date|months_between|months_between_rounded|years_between|\n+---+----------+--------------+----------------------+-------------+\n|  1|2019-07-01|   56.25806452|                 56.26|         4.69|\n|  2|2019-06-24|   56.51612903|                 56.52|         4.71|\n|  3|2019-08-24|   54.51612903|                 54.52|         4.54|\n+---+----------+--------------+----------------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round,lit\n",
    "from pyspark.sql.functions import months_between\n",
    "df.withColumn(\"months_between\", months_between(current_date(), col(\"date\")))\\\n",
    "    .withColumn(\"months_between_rounded\", round(months_between(current_date(), col(\"date\")), 2))\\\n",
    "    .withColumn(\"years_between\", round(months_between(current_date(), col(\"date\"))/ lit('12'), 2))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a4505f0-97dd-467b-bc0f-69c0d7ca0c48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n| id|      date|\n+---+----------+\n|  1|2019-07-01|\n|  2|2019-06-24|\n|  3|2019-08-24|\n+---+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "458c8133-18da-4cbb-b60b-ef8926a83eb2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Time and Date functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
