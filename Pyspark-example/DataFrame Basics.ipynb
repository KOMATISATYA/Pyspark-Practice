{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd5b1033-b0f6-414c-a4a5-7bbcc9b1d6f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Pysparl environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ab53112-5319-40ee-ba6a-895436c95f1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42eb921a-ca40-49e4-b80e-07b747e8ab6c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcdf8800-97c0-40be-b6b1-09c05d95b9e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Create a Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Sparksession\") \\\n",
    "        .master(\"local\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14336bae-e315-42c1-9a6d-fba1deaad28b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n|  Name|Age|\n+------+---+\n| Alice| 23|\n|   Bob| 30|\n|Mahesh| 21|\n+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Test the data\n",
    "data = [(\"Alice\", 23),(\"Bob\", 30),(\"Mahesh\", 21)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "#Let us show the data using show() method\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddc50186-3159-4d4e-b00a-7c625922b008",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03b9137f-c10d-4f9a-854d-6e6f741e1a5f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Difference between SparkContext and SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fda63ab-402b-4ad4-85d1-c4e8fab988e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  Spark Context :\n",
    "\n",
    "# ->Used to be the entry point for spark in the earlier \n",
    "#   versions say 1.x\n",
    "# ->Represents connection to spark cluster\n",
    "# ->Coordinates task execution across the cluster\n",
    "# ->Creates RDDs (Resilient Distributed Datasets)\n",
    "# ->Performs Transformations and defines actions.\n",
    "\n",
    "# Spark Session :\n",
    "\n",
    "# ->The entry point for Spark since the version 2.0 that provides simple Interaction.\n",
    "# ->Combines the Functionalities like HiveContext, SparkContext, SQLContext and StreamingContext.\n",
    "# ->Supports multiple Programming Languages like : Scala, Java, R and Python\n",
    "# ->Extends the functionality of Spark Context.\n",
    "# ->Supports Advanced abstractions like Datasets and DataFrames\n",
    "# ->Provides Data Source APIs, Machine Learning Algorithms and streaming capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef43cd9b-ff12-46e5-b096-256549a8438f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Creating pyspark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3731e2ff-9501-493e-84bf-061f0e0f4204",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "#Create a spark context variable\n",
    "sc=SparkContext(appName=\"SparkContext-Application\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b5bae58-bae8-4d10-8860-62a249c4f0a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Creating Spark Session with Manual Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdec6a64-0a20-4fa9-ad2d-81c94062ee40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Spark-Session-Manual-Config\") \\\n",
    "        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f668d38-3a54-4f8e-957f-5a9f1fdb8d98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=8083254535516109#setting/sparkui/0310-073150-aijadrla/driver-6521355015564303931\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=8083254535516109#setting/sparkui/0310-073150-aijadrla/driver-6521355015564303931\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "351c3877-1dac-4bec-9a52-b3198336946b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Shutdown the spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68a05a24-d6d9-41e4-8684-0f87e4956b94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### RDDs(Resilienyt Distributed Datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82a28239-df97-40d7-9d75-1d165de4c045",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Backbone of data processing in Spark\n",
    "# Distributed, Fault-tolerant, parallelizable data structure\n",
    "# Efficiently processes large datasets across the cluster\n",
    "# RDDs are immutable, distributed, resilient, lazily evaluated, fault-tolerant\n",
    "# Fault Tolerant operations may contain : map, filter, reduce, collect, count, save, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d5a63e9-ebfa-49b4-9ba7-bd3711043ba7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f57a2d82-2b6f-4837-aabd-5e235c35c1db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create new RDDs by applying computation/ Manipulation\n",
    "# Lazy Evaluation, Lineage Graph\n",
    "# Examples like map, filter, flatMap, reduceByKey, sortBy and join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e76e30c2-319a-4191-804b-3e17f9d03872",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31e1bbff-6127-4c9d-87dc-27ce426804da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Return Results or perform actions on RDD, triggering execution\n",
    "# Eager evaluation, data movement/ computation\n",
    "# Examples like collect, count, first, take, save, foreach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19235400-c6b4-4892-9fb3-851d3129bbcf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### How to Create RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d8c04b2-2517-46a6-8a2e-70068eddff2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "numbers = [1, 2, 3, 4, 5]\n",
    "rdd = spark.sparkContext.parallelize(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4b68c3d-dbdf-4237-b68e-c6aabf158e55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: [1, 2, 3, 4, 5]"
     ]
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fd006c4-fe55-4043-936a-9eda9bae54d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create an RDD from the List of Tuples\n",
    "employees = [(\"Ajay\", 8), (\"Raman\", 7), (\"Pratap\", 5), (\"Mohan\", 6), (\"Raman\", 3)]\n",
    "employees_rdd = spark.sparkContext.parallelize(employees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c11528d4-0e52-4230-ba33-cf7d0a24d29b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the employee tuples:\n('Ajay', 8)\n('Raman', 7)\n('Pratap', 5)\n('Mohan', 6)\n('Raman', 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"All the employee tuples:\")\n",
    "#Print the tuples in new line\n",
    "for i in employees_rdd.collect():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e92d53f-dd1a-4fad-8e85-8b60fd1b1d2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[9]: [('Ajay', 8), ('Raman', 7), ('Pratap', 5), ('Mohan', 6), ('Raman', 3)]"
     ]
    }
   ],
   "source": [
    "employees_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b561753-fbe8-4aa3-9b08-0313c02344e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### RDDs - Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "628ed0da-08fd-4c32-9624-74a2562186f4",
     "showTitle": true,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items in the RDD: 5\n"
     ]
    }
   ],
   "source": [
    "# Count() action is used to count the items in the RDD.\n",
    "rdd_count=rdd.count()\n",
    "print(\"number of items in the RDD:\",rdd_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f688bf84-4b46-4fe8-af89-05a2a051a804",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of items in the Employee RDD is :  5\n"
     ]
    }
   ],
   "source": [
    "employee_rdd_count = employees_rdd.count()\n",
    "print(\"The total number of items in the Employee RDD is : \", employee_rdd_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "749c5e5a-8f07-498d-9728-e76bb078878c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first item in the RDD is :  ('Ajay', 8)\n"
     ]
    }
   ],
   "source": [
    "# first() action returns the first action from the RDD.\n",
    "first_item = employees_rdd.first()\n",
    "print(\"The first item in the RDD is : \", first_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e62566d-eb62-4f3f-a906-d1f15779e0b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The elements from the RDD are : \n('Ajay', 8)\n('Raman', 7)\n('Pratap', 5)\n"
     ]
    }
   ],
   "source": [
    "# take() action is used to retrieve the n number of elements from the RDD\n",
    "elements_needed = employees_rdd.take(3)\n",
    "print(\"The elements from the RDD are : \")\n",
    "for i in elements_needed :\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2978957-18e0-45dc-b839-d01ddd071bd3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# foreach() action is used to print each element of the rdd\n",
    "employees_rdd.foreach(lambda x: print(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aeeca88-004b-4989-a726-e9dd00b657c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# foreach() action is used to print each element of the rdd\n",
    "employees_rdd.foreach(lambda x: print(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae67e5f8-13f9-48c7-af0c-4eae005ad2c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### RDDs-Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddc4d540-afa2-4b1c-8e02-31d230b718e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# In Transformations, the data will be changed but only returns result when any action is performed\n",
    "\n",
    "# Map Transformations are done to convert the name to uppercase\n",
    "mapped_rdd = employees_rdd.map(lambda x: (x[0].upper(), x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eda425d4-5677-4308-9701-432fa76dce0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD in Upper case : [('AJAY', 8), ('RAMAN', 7), ('PRATAP', 5), ('MOHAN', 6), ('RAMAN', 3)]\n"
     ]
    }
   ],
   "source": [
    "map_result = mapped_rdd.collect()\n",
    "print(\"RDD in Upper case :\",map_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4750c19-1ca1-472a-9ca5-3bd27e788feb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD with Experience of 7 years : [('Raman', 7)]\n"
     ]
    }
   ],
   "source": [
    "# filter Transformation : filter records based on any condition\n",
    "filtered_rdd = employees_rdd.filter(lambda x:x[1] == 7)\n",
    "\n",
    "filtered_result = filtered_rdd.collect()\n",
    "print(\"RDD with Experience of 7 years :\", filtered_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cb17b7a-45de-422e-bfca-def36890865d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[20]: [('Raman', 10), ('Pratap', 5), ('Ajay', 8), ('Mohan', 6)]"
     ]
    }
   ],
   "source": [
    "# ReduceBy Key : Calculate the total experience for each name\n",
    "reduced_rdd = employees_rdd.reduceByKey(lambda x,y: x + y)\n",
    "reduced_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e587669-4304-426b-a28f-a286d600a9a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RDD in Ascending Order : [('Raman', 3), ('Pratap', 5), ('Mohan', 6), ('Raman', 7), ('Ajay', 8)]\nThe RDD in Descending Order : [('Ajay', 8), ('Raman', 7), ('Mohan', 6), ('Pratap', 5), ('Raman', 3)]\n"
     ]
    }
   ],
   "source": [
    "# sortBy Transformation : This returns the data arranged in ascending or descending order\n",
    "sorted_rdd_asc = employees_rdd.sortBy(lambda x: x[1], ascending = True)\n",
    "print(\"The RDD in Ascending Order :\",sorted_rdd_asc.collect())\n",
    "\n",
    "sorted_rdd_desc = employees_rdd.sortBy(lambda x: x[1], ascending = False)\n",
    "print(\"The RDD in Descending Order :\",sorted_rdd_desc.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c50066c5-3e14-468d-9c8a-48704ac7431d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DataFrame Basics",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
