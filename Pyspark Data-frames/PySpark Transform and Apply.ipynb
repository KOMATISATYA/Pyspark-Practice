{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4617437-ba93-48ae-9e38-cc9ef7398350",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61a7474e-460a-4b64-a460-9edc0cf8c649",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- CourseName: string (nullable = true)\n |-- fee: long (nullable = true)\n |-- discount: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "            .appName('Transform in Pyspark') \\\n",
    "            .getOrCreate()\n",
    "\n",
    "# Prepare Data\n",
    "simpleData = ((\"Java\",4000,5), \\\n",
    "    (\"Python\", 4600,10),  \\\n",
    "    (\"Scala\", 4100,15),   \\\n",
    "    (\"Scala\", 4500,15),   \\\n",
    "    (\"PHP\", 3000,20),  \\\n",
    "  )\n",
    "columns= [\"CourseName\", \"fee\", \"discount\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3aa455a-62cd-4be3-be8f-e7c1ee5d6da2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f20a81d-5ef6-48d6-8517-558c195460dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Custom_function_1\n",
    "from pyspark.sql.functions import upper\n",
    "def to_upper_str_columns(df):\n",
    "    return df.withColumn(\"Course_name\",upper(df.CourseName))\n",
    "#Custom_function_2\n",
    "def reduce_price(df,reduceBy):\n",
    "    return df.withColumn(\"new_fee\",df.fee-reduceBy)\n",
    "\n",
    "#custom_function_3\n",
    "def apply_discount(df):\n",
    "    return df.withColumn(\"discount_fee\",df.new_fee-(df.new_fee*df.discount)/100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72775cb0-1aa5-43cc-a6fd-d44c20e18c38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2=df.transform(to_upper_str_columns)\\\n",
    "    .transform(reduce_price,1000)\\\n",
    "        .transform(apply_discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95208f87-520c-446c-be7c-eda48909d189",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------+-----------+-------+------------+\n|CourseName| fee|discount|Course_name|new_fee|discount_fee|\n+----------+----+--------+-----------+-------+------------+\n|      Java|4000|       5|       JAVA|   3000|      2850.0|\n|    Python|4600|      10|     PYTHON|   3600|      3240.0|\n|     Scala|4100|      15|      SCALA|   3100|      2635.0|\n|     Scala|4500|      15|      SCALA|   3500|      2975.0|\n|       PHP|3000|      20|        PHP|   2000|      1600.0|\n+----------+----+--------+-----------+-------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c703997-3f77-4374-a774-338d248fd1a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------+-----------+-------+------------+\n|CourseName| fee|discount|Course_name|new_fee|discount_fee|\n+----------+----+--------+-----------+-------+------------+\n|      Java|4000|       5|       JAVA|   3000|      2850.0|\n|    Python|4600|      10|     PYTHON|   3600|      3240.0|\n|     Scala|4100|      15|      SCALA|   3100|      2635.0|\n|     Scala|4500|      15|      SCALA|   3500|      2975.0|\n|       PHP|3000|      20|        PHP|   2000|      1600.0|\n+----------+----+--------+-----------+-------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "def sel_col(df):\n",
    "    return df.select(df2.columns)\n",
    "df3=df2.transform(sel_col)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a776bf9a-76b8-4ef5-85b9-d28d54cf9bef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Array Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23d6e010-04fa-44be-825d-bfd5f7108b50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Name: string (nullable = true)\n |-- Languages1: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- Languages2: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame with Array\n",
    "data = [\n",
    " (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]),\n",
    " (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]),\n",
    " (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"])\n",
    "]\n",
    "df = spark.createDataFrame(data=data,schema=[\"Name\",\"Languages1\",\"Languages2\"])\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fbcbfef-36be-4138-80b5-c30440a497ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n|        languages1|\n+------------------+\n|[JAVA, SCALA, C++]|\n|[SPARK, JAVA, C++]|\n|      [CSHARP, VB]|\n+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "from pyspark.sql.functions import transform\n",
    "df.select(transform(\"Languages1\",lambda x: upper(x)).alias(\"languages1\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e254c94a-5357-4f31-9ff3-340a5d3859b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcb2db92-e1c2-471d-abbd-83e75ccddd53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n| no|        name|\n+---+------------+\n|  1|  john jones|\n|  2|tracey smith|\n|  3| amy sanders|\n+---+------------+\n\n"
     ]
    }
   ],
   "source": [
    "columns=['no','name']\n",
    "data=[('1','john jones'),\n",
    "      ('2','tracey smith'),\n",
    "      ('3','amy sanders')]\n",
    "df=spark.createDataFrame(data=data,schema=columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c86d8226-a25e-4368-b81f-4bdbd05b26f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Applying Function using withColumn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9d0cf4b-ebc2-4af1-84e6-9f228f94852b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------------+\n| no|        name|  upper_name|\n+---+------------+------------+\n|  1|  john jones|  JOHN JONES|\n|  2|tracey smith|TRACEY SMITH|\n|  3| amy sanders| AMY SANDERS|\n+---+------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "df.withColumn(\"upper_name\",upper(df.name)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71634842-e8ab-4fb6-933a-44b3d05590a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Applying with select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c0db314-1c6b-4b89-900b-5d7749cbe36e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------------+\n| no|        name| upper(name)|\n+---+------------+------------+\n|  1|  john jones|  JOHN JONES|\n|  2|tracey smith|TRACEY SMITH|\n|  3| amy sanders| AMY SANDERS|\n+---+------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(\"no\",\"name\",upper(df.name)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0429bbae-06f1-4888-a7b7-6ef839935ab0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### With Spark Sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35738b21-a99b-4c6c-b115-b7e00f61216c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------------+\n| no|        name| upper(name)|\n+---+------------+------------+\n|  1|  john jones|  JOHN JONES|\n|  2|tracey smith|TRACEY SMITH|\n|  3| amy sanders| AMY SANDERS|\n+---+------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"tem\")\n",
    "df_sql=spark.sql(\"\"\"\n",
    "                 select no,name,upper(name) from tem\n",
    "                 \"\"\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f593e2e4-8a51-4143-9c1e-e88c39abe1fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Create a custom function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5ad2a96-d507-4340-9d31-6295a077cab9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def upper_case(str):\n",
    "    return str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17de4b7f-53ee-4b14-a82f-05b098eda76e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert function to UDF\n",
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql.types import StringType\n",
    "upperCaseUDF=udf(lambda x:upper_case(x),StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46d417cb-bdd0-4dd0-ac0e-38836fc2db36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------------+\n| no|        name|  naameUpper|\n+---+------------+------------+\n|  1|  john jones|  JOHN JONES|\n|  2|tracey smith|TRACEY SMITH|\n|  3| amy sanders| AMY SANDERS|\n+---+------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Custom UDF with select\n",
    "df.select(\"no\",\"name\",upperCaseUDF(col(\"name\")).alias(\"naameUpper\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e4266cd-38ff-4f8e-8641-8b07ff81b4e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------------+\n| no|        name|       upper|\n+---+------------+------------+\n|  1|  john jones|  JOHN JONES|\n|  2|tracey smith|TRACEY SMITH|\n|  3| amy sanders| AMY SANDERS|\n+---+------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# custom udf withColumn\n",
    "df.withColumn(\"upper\",upperCaseUDF(col(\"name\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f49cf2e-dd11-4585-b6ef-8ef3bef1ba63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------------+\n| no|        name|   upperCase|\n+---+------------+------------+\n|  1|  john jones|  JOHN JONES|\n|  2|tracey smith|TRACEY SMITH|\n|  3| amy sanders| AMY SANDERS|\n+---+------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Custom function with spark sql\n",
    "spark.udf.register(\"upperCaseUDF\",upperCaseUDF)\n",
    "df.createOrReplaceTempView('udfTable')\n",
    "spark.sql(\"\"\"\n",
    "          SELECT no, name, upperCaseUDF(Name) as upperCase from udfTable\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "967fa4d3-9ff0-4cb3-922f-897d320ae2f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Pyspark pandas Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "204dda64-20e5-49e7-8b31-b93cae51fe3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Fee  Discount\n0  20000.0      1000\n1  40000.0      2500\n2  25000.0      1500\n3  22000.0      1200\n4      NaN      3000\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "technologies = ({\n",
    "    'Fee' : [20000, 40000, 25000, 22000, np.NaN],\n",
    "    'Discount' : [1000, 2500, 1500, 1200, 3000]\n",
    "})\n",
    "\n",
    "psdf = pd.DataFrame(technologies)\n",
    "print(psdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbfcdbea-7e47-42f6-98dc-b82021840e1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add(data):\n",
    "    return data[0]+data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b7e6239-b4ef-43da-a6c7-37f0417b006d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "addDF=psdf.apply(add,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff980351-e98e-4a8c-b574-9840265fb59b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    21000.0\n1    42500.0\n2    26500.0\n3    23200.0\n4        NaN\ndtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(addDF)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark Transform and Apply",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
