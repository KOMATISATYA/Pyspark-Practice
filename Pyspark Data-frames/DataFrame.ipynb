{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7003adc-2b67-4dd9-b19b-a97f62610f5e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Pyspark Environmental Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97d18b7c-ef61-4075-8e9a-609c82574f3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2516fffe-103e-4700-89ab-35225ddb9874",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Create sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f4497de-591b-4d63-b17d-23d65286a7f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "#Create a Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Sparksession\") \\\n",
    "        .master(\"local\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f749794c-5725-496f-b0a9-7d3aecba8178",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=8083254535516109#setting/sparkui/0310-073150-aijadrla/driver-5086910750393131779\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=8083254535516109#setting/sparkui/0310-073150-aijadrla/driver-5086910750393131779\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49da1413-9327-41f4-ad2c-46f549e52af0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Creating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71042f7d-8187-4461-b344-8ebbd9013421",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmptyRDD[0] at emptyRDD at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "# Creating an Empty RDD\n",
    "rdd=spark.sparkContext.emptyRDD()\n",
    "print(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b04b371-e5a1-45b5-953b-2507000df9f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType\n",
    "schema=StructType([\n",
    "    StructField('First Name',StringType(),True),\n",
    "    StructField('Middle Name',StringType(),True),\n",
    "    StructField('Last Name',StringType(),True)\n",
    "])\n",
    "df=spark.createDataFrame(rdd,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9fdd899-0099-47aa-9c02-01a9990c1d8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- First Name: string (nullable = true)\n |-- Middle Name: string (nullable = true)\n |-- Last Name: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b690fce5-3f79-4318-a2ca-f82114bf5f04",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Converting an existing RDD to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2cdfe95-252d-460f-9d70-1519e0ea9c2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[First Name: string, Middle Name: string, Last Name: string]\n"
     ]
    }
   ],
   "source": [
    "df1=rdd.toDF(schema)\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "278d7593-18df-49d9-8c05-a83795b953c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Creating an empty DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "667019dc-4539-4a5f-9e76-685d27cf1ca2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- First Name: string (nullable = true)\n |-- Middle Name: string (nullable = true)\n |-- Last Name: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df2=spark.createDataFrame([],schema)\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aa78957-d920-4e82-8228-7aa0f63a4883",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Creating Empty DataFrame without any schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5f517c5-c225-4a3d-b7b3-e58a3f2c92c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[]\n"
     ]
    }
   ],
   "source": [
    "df3=spark.createDataFrame([],StructType([]))\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d474173-f437-42c2-b9c9-360d217e949f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Converting pyspark Rdd to Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c5f7c28-ea56-49ac-81db-f00ed58166d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.Creating Pyspaark RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6259b3f6-4714-4afc-a7bc-17dea5146fd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dept=[(\"Finanace\",300),(\"Marketing\",450),(\"Promotions\",250)]\n",
    "rdd=spark.sparkContext.parallelize(dept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9479f695-b273-4f5b-b971-eee65ecd4553",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.Convert pyspark Rdd to DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49ff3276-239d-44d3-835d-eefde9e4e14e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### a.Using toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf985a79-ab47-4d68-9474-65ddd83a070a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- _1: string (nullable = true)\n |-- _2: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df=rdd.toDF()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e66d19e-193d-4783-9f94-61d57bc9f22d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Sector: string (nullable = true)\n |-- Id: long (nullable = true)\n\n+----------+---+\n|    Sector| Id|\n+----------+---+\n|  Finanace|300|\n| Marketing|450|\n|Promotions|250|\n+----------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# With desired column names\n",
    "columns=[\"Sector\",\"Id\"]\n",
    "df=rdd.toDF(columns)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5044c8e-2842-4bee-b98f-4100a364bef8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### b.Using createDataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78e1ebc6-a3df-44bc-931f-c8a4aa11ff55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n|    Sector| Id|\n+----------+---+\n|  Finanace|300|\n| Marketing|450|\n|Promotions|250|\n+----------+---+\n\n"
     ]
    }
   ],
   "source": [
    "df1=spark.createDataFrame(rdd,schema=columns)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ff2eb5c-9578-448c-bbb5-2d2ea62c6798",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Converting DataFrame into Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1d40fa9-415d-4a1e-9b16-70e99b091b5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Sector   Id\n0    Finanace  300\n1   Marketing  450\n2  Promotions  250\n"
     ]
    }
   ],
   "source": [
    "df_pandas=df.toPandas()\n",
    "print(df_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8f84994-98b2-4698-9298-5281b306ce9d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cecd68c-c8fa-40c3-9a99-938af94e85bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columns = [\"Seqno\",\"Quote\"]\n",
    "data = [(\"1\", \"Be the change that you wish to see in the world\"),\n",
    "    (\"2\", \"Everyone thinks of changing the world, but no one thinks of changing himself.\"),\n",
    "    (\"3\", \"The purpose of our lives is to be happy.\"),\n",
    "    (\"4\", \"Be cool.\")]\n",
    "df = spark.createDataFrame(data,columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab534d25-9d41-4ed4-821e-a5fd2720be86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n|Seqno|               Quote|\n+-----+--------------------+\n|    1|Be the change tha...|\n|    2|Everyone thinks o...|\n|    3|The purpose of ou...|\n|    4|            Be cool.|\n+-----+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# display the contents upto 20 characters in the values of the columns\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "948d2298-18ca-4e23-837d-962c70103734",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n|Seqno|               Quote|\n+-----+--------------------+\n|    1|Be the change tha...|\n|    2|Everyone thinks o...|\n|    3|The purpose of ou...|\n+-----+--------------------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "# If you pass the value as a parameter to the show(), That number of rows will be displayed\n",
    "df.show(3)   # here n = 3, stands for the number of rows to be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc646836-9111-44b8-ab9d-eae8a4d061e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------------------+\n|Seqno|                                   Quote|\n+-----+----------------------------------------+\n|    1|Be the change that you wish to see in...|\n|    2|Everyone thinks of changing the world...|\n|    3|The purpose of our lives is to be happy.|\n|    4|                                Be cool.|\n+-----+----------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# When the truncate value is set to any value(viz, 40). Only 40 characters is displayed from the characters of the column value\n",
    "df.show(truncate = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f44f39b-a6e5-4954-8960-62ab564e0373",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------------------------------------------------+\n|Seqno|Quote                                                                        |\n+-----+-----------------------------------------------------------------------------+\n|1    |Be the change that you wish to see in the world                              |\n|2    |Everyone thinks of changing the world, but no one thinks of changing himself.|\n|3    |The purpose of our lives is to be happy.                                     |\n+-----+-----------------------------------------------------------------------------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "# When the truncate value is set to False, The Characters in the columns will be completely displayed\n",
    "df.show(3, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c334d869-6fbc-4999-b980-820ba9a28771",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------------------------------------------\n Seqno | 1                                                                             \n Quote | Be the change that you wish to see in the world                               \n-RECORD 1------------------------------------------------------------------------------\n Seqno | 2                                                                             \n Quote | Everyone thinks of changing the world, but no one thinks of changing himself. \n-RECORD 2------------------------------------------------------------------------------\n Seqno | 3                                                                             \n Quote | The purpose of our lives is to be happy.                                      \nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "# If you want to display the details vertically, we use the vertical = true\n",
    "df.show(n = 3, truncate = False, vertical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "872b66b3-8f55-4e53-ae71-e8fc7adbc3ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### StructType and StructField"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb27bec2-2aec-4b72-8f2b-16ced00d0e0c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Define columns with schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9e5c049-6456-4d47-bc2e-b42ce9894816",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from pyspark.sql.types import StructType, StringType, StructField, IntegerType, FloatType\n",
    "\n",
    "# Create schema\n",
    "schema = StructType([\n",
    "    StructField('Student Id', IntegerType(), True),\n",
    "    StructField('First Name', StringType(), True),\n",
    "    StructField('Last Name', StringType(), True),\n",
    "    StructField('Attendance', FloatType(), True)\n",
    "])\n",
    "\n",
    "# Create Data to insert\n",
    "data = [\n",
    "    (1, \"Satya\", \"Komati\", 89.07),\n",
    "    (2, \"Adhi\", \"\", 73.89),\n",
    "    (3, \"Pavi\", \"Gorrela\", 92.676),\n",
    "    (4, \"Vinod\", \"Gorrela\", 87.273),\n",
    "    (5, \"Vishal\", \"\", 84.30)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2413205d-8129-42e4-8590-8bcb91f5d09b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Student Id: integer (nullable = true)\n |-- First Name: string (nullable = true)\n |-- Last Name: string (nullable = true)\n |-- Attendance: float (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Creating dataframe\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Viewing the Schema of the Data frame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b93af4a-002f-463a-a69e-b362dd553531",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+----------+\n|Student Id|First Name|Last Name|Attendance|\n+----------+----------+---------+----------+\n|         1|     Satya|   Komati|     89.07|\n|         2|      Adhi|         |     73.89|\n|         3|      Pavi|  Gorrela|    92.676|\n|         4|     Vinod|  Gorrela|    87.273|\n|         5|    Vishal|         |      84.3|\n+----------+----------+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Showing the data from the database\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "639a7345-8ea6-49f2-a49b-ab8273173581",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Nesting the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e66169f3-66a7-44d3-b10d-ed532adb9952",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Nesting the data\n",
    "structure_data = [\n",
    "    (1, (\"Dhilli\", \"\"), 23.40),\n",
    "    (2, (\"Rolex\", \"Watson\"), 85.34),\n",
    "    (3, (\"Amar\",\"\"), 12.43),\n",
    "    (4, (\"Leo\", \"Das\"), 98.23),\n",
    "    (5, (\"Vikram\", \"Iyer\"), 2.09)\n",
    "]\n",
    "\n",
    "#Nested Schema Structure\n",
    "structured_schema=StructType([\n",
    "    StructField(\"Criminal_Id\",IntegerType(),True),\n",
    "    StructField(\"name\",StructType([\n",
    "        StructField(\"first_name\",StringType(),True),\n",
    "        StructField(\"last_name\",StringType(),True)\n",
    "    ])),\n",
    "    StructField(\"Criminal_Percent\",FloatType(),True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e88f2be6-947c-46f5-8c9e-9519599290bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lcu_df=spark.createDataFrame(structure_data,structured_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "225d606b-f1e3-4971-94d1-37500e9d5697",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+----------------+\n|Criminal_Id|name           |Criminal_Percent|\n+-----------+---------------+----------------+\n|1          |{Dhilli, }     |23.4            |\n|2          |{Rolex, Watson}|85.34           |\n|3          |{Amar, }       |12.43           |\n|4          |{Leo, Das}     |98.23           |\n|5          |{Vikram, Iyer} |2.09            |\n+-----------+---------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "lcu_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab943f1d-7a63-4763-9824-281dc0206873",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Updating the structure of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86d50417-a6dd-48c0-928a-93c81d8bd341",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Student Id: integer (nullable = true)\n |-- First Name: string (nullable = true)\n |-- Last Name: string (nullable = true)\n |-- Attendance: float (nullable = true)\n |-- Other info: struct (nullable = false)\n |    |-- id: integer (nullable = true)\n |    |-- first_name: string (nullable = true)\n |    |-- last_name: string (nullable = true)\n |    |-- percent: float (nullable = true)\n |    |-- Eligibility: string (nullable = false)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,when,struct\n",
    "\n",
    "#Updating the schema\n",
    "updated_df=df.withColumn(\"Other info\",\n",
    "        struct(col(\"Student Id\").alias(\"id\"),\n",
    "               col(\"First Name\").alias(\"first_name\"),\n",
    "               col(\"Last Name\").alias(\"last_name\"),\n",
    "               col(\"Attendance\").alias(\"percent\"),\n",
    "               when(col(\"Attendance\").cast(IntegerType())<75,\"Not Eligible\").otherwise(\"Eligible\").alias(\"Eligibility\")\n",
    "         ))\n",
    "updated_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6e637cf-a5da-413e-8941-e328db6938e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+----------+-------------------------------------+\n|Student Id|First Name|Last Name|Attendance|Other info                           |\n+----------+----------+---------+----------+-------------------------------------+\n|1         |Satya     |Komati   |89.07     |{1, Satya, Komati, 89.07, Eligible}  |\n|2         |Adhi      |         |73.89     |{2, Adhi, , 73.89, Not Eligible}     |\n|3         |Pavi      |Gorrela  |92.676    |{3, Pavi, Gorrela, 92.676, Eligible} |\n|4         |Vinod     |Gorrela  |87.273    |{4, Vinod, Gorrela, 87.273, Eligible}|\n|5         |Vishal    |         |84.3      |{5, Vishal, , 84.3, Eligible}        |\n+----------+----------+---------+----------+-------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "updated_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f77a226e-66c2-46dc-9504-a99c3cc5c53b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Adding the new columns to the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8984a18-73d8-4ffa-8e79-a324e3d6786d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "updated_df_with_eligibility=df.withColumn(\"Eligibility\",when(col(\"Attendance\").cast(IntegerType())<75,\"No\").otherwise(\"Yes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d139dfd2-a5d0-4c52-9b95-61f2bc5121ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Student Id: integer (nullable = true)\n |-- First Name: string (nullable = true)\n |-- Last Name: string (nullable = true)\n |-- Attendance: float (nullable = true)\n |-- Eligibility: string (nullable = false)\n\n"
     ]
    }
   ],
   "source": [
    "updated_df_with_eligibility.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffc776f0-8161-43b4-a504-8cf1e01f3992",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+----------+-----------+\n|Student Id|First Name|Last Name|Attendance|Eligibility|\n+----------+----------+---------+----------+-----------+\n|1         |Satya     |Komati   |89.07     |Yes        |\n|2         |Adhi      |         |73.89     |No         |\n|3         |Pavi      |Gorrela  |92.676    |Yes        |\n|4         |Vinod     |Gorrela  |87.273    |Yes        |\n|5         |Vishal    |         |84.3      |Yes        |\n+----------+----------+---------+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "updated_df_with_eligibility.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a39acbe-4473-4fd1-a358-0f5641a2d8cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Using Sql Array and Map Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f3fa595-7a76-4398-881a-d75d39874e3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType,MapType\n",
    "arrayAndMapSchema=StructType([\n",
    "    StructField(\"First Name\",StringType(),True),\n",
    "    StructField(\"Last Name\", StringType(), True),\n",
    "    StructField(\"Attendance\", FloatType(), True),\n",
    "    StructField(\"Hobbies\", ArrayType(StringType()), True),\n",
    "    StructField(\"Properties\", MapType(StringType(), StringType()), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f709741-7b6b-497e-a2a0-822da83b0139",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"fields\":[{\"metadata\":{},\"name\":\"Student Id\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"First Name\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Last Name\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Attendance\",\"nullable\":true,\"type\":\"float\"}],\"type\":\"struct\"}\n"
     ]
    }
   ],
   "source": [
    "# If there are more number of columns,we can use the schema.json() method to print the Schema in json format.\n",
    "print(df.schema.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06aedc59-5984-48f5-950a-66bd59226aab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "struct<Student Id:int,First Name:string,Last Name:string,Attendance:float>\n"
     ]
    }
   ],
   "source": [
    "# To print it in the simplest format, we can use the simpleString() method\n",
    "print(df.schema.simpleString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a1fddd6-3cf1-47cd-9ff1-14af15b94d89",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "963ab0e3-9c40-449a-bd55-f3ec3d839ec4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=df.withColumnRenamed(\"First Name\",\"first_name\")\\\n",
    "    .withColumnRenamed(\"Last Name\",\"last_name\")\\\n",
    "        .withColumnRenamed(\"Student Id\",\"student_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe5fef33-7bcd-4840-9967-49aa49a19744",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- student_id: integer (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- Attendance: float (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8be3c946-21ca-4a17-8a24-953705675fd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "dataDF = [(('James','','Smith'),'1991-04-01','M',3000),\n",
    "  (('Michael','Rose',''),'2000-05-19','M',4000),\n",
    "  (('Robert','','Williams'),'1978-09-05','M',4000),\n",
    "  (('Maria','Anne','Jones'),'1967-12-01','F',4000),\n",
    "  (('Jen','Mary','Brown'),'1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('dob', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('salary', IntegerType(), True)\n",
    "         ])\n",
    "\n",
    "data_df = spark.createDataFrame(data = dataDF, schema = schema)\n",
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56477e8a-6536-47bf-9b24-b3f6f19b4f58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- newCol1: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- newCol2: string (nullable = true)\n |-- newCol3: string (nullable = true)\n |-- newCol4: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "newColumns=[\"newCol1\",\"newCol2\",\"newCol3\",\"newCol4\"]\n",
    "data_df.toDF(*newColumns).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1d3e859-9517-4753-a857-851381bb1f12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[43]: ['name', 'dob', 'gender', 'salary']"
     ]
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd476a38-cdc5-435b-8da3-e98f1c5dcda6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Column Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d2d094b-a0cb-43ee-b396-0bfb3d063dc0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "colObj=lit(\"sparkbyexamples.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51f52027-15fc-4687-bd58-a1299ba7e38f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Criminal_Id: integer (nullable = true)\n |-- name: struct (nullable = true)\n |    |-- first_name: string (nullable = true)\n |    |-- last_name: string (nullable = true)\n |-- Criminal_Percent: float (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "lcu_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "291d7e49-ba68-47c3-95d3-f5b4cf5c47ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|Attendance|\n+----------+\n|     89.07|\n|     73.89|\n|    92.676|\n|    87.273|\n|      84.3|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Using . operator\n",
    "df.select(df.Attendance).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddb2dfd7-5b76-4ce1-9090-3b600d422a50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n|first_name|last_name|\n+----------+---------+\n|     Satya|   Komati|\n|      Adhi|         |\n|      Pavi|  Gorrela|\n|     Vinod|  Gorrela|\n|    Vishal|         |\n+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Using df[\"Column Name\"]\n",
    "df.select(df[\"first_name\"], df[\"last_name\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d474d233-d03d-4ff7-aecf-3e6a506f7e41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n|student_id|first_name|\n+----------+----------+\n|         1|     Satya|\n|         2|      Adhi|\n|         3|      Pavi|\n|         4|     Vinod|\n|         5|    Vishal|\n+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Using col function\n",
    "from pyspark.sql.functions import col\n",
    "df.select(col(\"student_id\"),col(\"first_name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe6e7537-59e8-45cf-9074-89ec788d7173",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|first_name|\n+----------+\n|    Dhilli|\n|     Rolex|\n|      Amar|\n|       Leo|\n|    Vikram|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "lcu_df.select(lcu_df[\"name.first_name\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "590b35f2-66e3-4046-a111-489c5ab8a3f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Creating a DataFrame using Row Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d5f954e-4578-430e-9009-7ecf58fcca2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "data=[Row(name=\"James\",prop=Row(hair=\"black\",eye=\"brown\")),\n",
    "      Row(name=\"Rahul\",prop=Row(hair=\"blue\",eye=\"reddish\"))]\n",
    "df_drop=spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebd9a3b9-a979-45bd-ac91-19dd12f645e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n| hair|  eye|\n+-----+-----+\n|black|brown|\n+-----+-----+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "df_drop.select(col(\"prop.*\")).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d8ec111-dc23-4161-9c00-c6dc73d17073",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Arithmetic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a211d08-a8f2-444e-a0f6-6e4388d1cad2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data=[(150,23,8),(180,43,5),(129,85,1)]\n",
    "ndf=spark.createDataFrame(data).toDF(\"col1\",\"col2\",\"col3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d4c46c7-7bf8-4809-8a85-5681152913ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n|col1|col2|col3|\n+----+----+----+\n| 150|  23|   8|\n| 180|  43|   5|\n| 129|  85|   1|\n+----+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "ndf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4784162-e781-4e28-aee2-ec3eedd129fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n|Sum|\n+---+\n|173|\n|223|\n|214|\n+---+\n\n+-------------+\n|(col1 - col2)|\n+-------------+\n|          127|\n|          137|\n|           44|\n+-------------+\n\n+-------------+\n|(col1 * col3)|\n+-------------+\n|         1200|\n|          900|\n|          129|\n+-------------+\n\n+-------------+\n|(col1 / col3)|\n+-------------+\n|        18.75|\n|         36.0|\n|        129.0|\n+-------------+\n\n+-------------+\n|(col1 % col3)|\n+-------------+\n|            6|\n|            0|\n|            0|\n+-------------+\n\n----------------------------------------------------------------------\n+-------------+\n|(col1 < col3)|\n+-------------+\n|        false|\n|        false|\n|        false|\n+-------------+\n\n+-------------+\n|(col1 > col3)|\n+-------------+\n|         true|\n|         true|\n|         true|\n+-------------+\n\n+--------------+\n|(col1 <= col3)|\n+--------------+\n|         false|\n|         false|\n|         false|\n+--------------+\n\n+--------------+\n|(col1 >= col3)|\n+--------------+\n|          true|\n|          true|\n|          true|\n+--------------+\n\n+-------------+\n|(col1 = col3)|\n+-------------+\n|        false|\n|        false|\n|        false|\n+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "ndf.select((ndf[\"col1\"] + ndf[\"col2\"]).alias(\"Sum\")).show()\n",
    "ndf.select(ndf[\"col1\"] - ndf[\"col2\"]).show()\n",
    "ndf.select(ndf[\"col1\"] * ndf[\"col3\"]).show()\n",
    "ndf.select(ndf[\"col1\"] / ndf[\"col3\"]).show()\n",
    "ndf.select(ndf[\"col1\"] % ndf[\"col3\"]).show()\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "ndf.select(ndf[\"col1\"] < ndf[\"col3\"]).show()\n",
    "ndf.select(ndf[\"col1\"] > ndf[\"col3\"]).show()\n",
    "ndf.select(ndf[\"col1\"] <= ndf[\"col3\"]).show()\n",
    "ndf.select(ndf[\"col1\"] >= ndf[\"col3\"]).show()\n",
    "ndf.select(ndf[\"col1\"] == ndf[\"col3\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e808aa92-47ee-46ae-aa91-38d698141c12",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Pyspark column functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9169b86-5165-45d9-b806-5367748fdbb3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n|substring(last_name, 1, 2)|\n+--------------------------+\n|                        Ko|\n|                          |\n|                        Go|\n|                        Go|\n|                          |\n+--------------------------+\n\n+------------------------+\n|startswith(last_name, K)|\n+------------------------+\n|                    true|\n|                   false|\n|                   false|\n|                   false|\n|                   false|\n+------------------------+\n\n+----------------------+\n|endswith(last_name, a)|\n+----------------------+\n|                 false|\n|                 false|\n|                  true|\n|                  true|\n|                 false|\n+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# substr (starting Position, length of the substring you wish to return) \n",
    "df.select(col(\"last_name\").substr(1,2)).show()\n",
    "\n",
    "# # starts with checks whether the item starts with a specific character or not\n",
    "df.select(col(\"last_name\").startswith('K')).show()\n",
    "\n",
    "# # ends with checks whether the item starts ends with a specific character or not\n",
    "df.select(col(\"last_name\").endswith('a')).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DataFrame",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
