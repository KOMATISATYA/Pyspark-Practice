{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "209cd0e3-01ad-4240-b414-c8f8e5ef0044",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Drop operation Rows and Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "037aaa30-24b1-4ffc-abe7-039f7208ddcf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74649714-47c0-4951-bbb0-7a0c32ff53f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8a6c8b3-4dd0-4b55-b838-f1a2fd688f78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data\n",
    "data = [(\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600), \\\n",
    "    (\"Robert\", \"Sales\", 4100), \\\n",
    "    (\"Maria\", \"Finance\", 3000), \\\n",
    "    (\"James\", \"Sales\", 3000), \\\n",
    "    (\"Scott\", \"Finance\", 3300), \\\n",
    "    (\"Jen\", \"Finance\", 3900), \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000), \\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  ]\n",
    "\n",
    "# Create DataFrame\n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25182caa-ffcc-42c3-a2bb-115d3c52f16c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|        James|     Sales|  3000|\n|      Michael|     Sales|  4600|\n|       Robert|     Sales|  4100|\n|        Maria|   Finance|  3000|\n|        Scott|   Finance|  3300|\n|          Jen|   Finance|  3900|\n|         Jeff| Marketing|  3000|\n|        Kumar| Marketing|  2000|\n|         Saif|     Sales|  4100|\n+-------------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# distinct() method is used to display the rows by elimination of the duplications\n",
    "df.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81eb6f0f-94b1-4c2e-8932-e7f835471070",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of the rows in data frame: 10\nCount of rows without dupicates: 9\n"
     ]
    }
   ],
   "source": [
    "print(\"Count of the rows in data frame:\", df.count())\n",
    "print(\"Count of rows without dupicates:\",df.distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d32c8f2d-683c-4381-8be3-f1385e3548c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Dropping duplictes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc05f301-624b-46d7-a72c-bb4e3770becf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count the number of rows: 9\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|        James|     Sales|  3000|\n|      Michael|     Sales|  4600|\n|       Robert|     Sales|  4100|\n|        Maria|   Finance|  3000|\n|        Scott|   Finance|  3300|\n|          Jen|   Finance|  3900|\n|         Jeff| Marketing|  3000|\n|        Kumar| Marketing|  2000|\n|         Saif|     Sales|  4100|\n+-------------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# dropDuplictes() method used to drop the duplicates\n",
    "df2=df.dropDuplicates()\n",
    "print(\"Count the number of rows:\",df2.count())\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2193950c-3286-4f1b-a272-7e28b0ba25b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Checking count of individual columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02a8de13-7d4a-48e8-9491-4d20f78e92f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distinct count of 'employee_name' 9\nThe distinct count of 'department' 3\nThe distinct count of 'salary' 6\n"
     ]
    }
   ],
   "source": [
    "print(\"The distinct count of 'employee_name'\",df.select(\"employee_name\").distinct().count())\n",
    "print(\"The distinct count of 'department'\",df.select(\"department\").distinct().count())\n",
    "print(\"The distinct count of 'salary'\",df.select(\"salary\").distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21e0308f-ef69-4aed-8e80-c6b4125f9bfd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Drop duplicates for selected columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e063218-8327-47e6-b8e9-92c57d0527ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|        Maria|   Finance|  3000|\n|        Scott|   Finance|  3300|\n|          Jen|   Finance|  3900|\n|        Kumar| Marketing|  2000|\n|         Jeff| Marketing|  3000|\n|        James|     Sales|  3000|\n|       Robert|     Sales|  4100|\n|      Michael|     Sales|  4600|\n+-------------+----------+------+\n\nThe count of the Columns after dropping duplicates of 2 columns :  8\n"
     ]
    }
   ],
   "source": [
    "# Remove Duplicates for selected columns using dropDuplicates() method\n",
    "df_cols2 = df.dropDuplicates(['Department', 'Salary'])\n",
    "df_cols2.show()\n",
    "print(\"The count of the Columns after dropping duplicates of 2 columns : \", df_cols2.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbbd9961-3287-45ed-9c95-4ee57d85877f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Drop Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee069bd9-58d3-454a-8368-2faa581fff5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- location: string (nullable = true)\n |-- salary: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "simpleData = ((\"James\",\"\",\"Smith\",\"36636\",\"NewYork\",3100), \\\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"California\",4300), \\\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"Florida\",1400), \\\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"Florida\",5500), \\\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"34561\",\"NewYork\",3000) \\\n",
    "  )\n",
    "columns= [\"firstname\",\"middlename\",\"lastname\",\"id\",\"location\",\"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0b4bb9d-5abf-4d0d-9151-5d38891ecdfb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Dropping single columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1743e5a-fc9a-4ab3-ac56-deb15d5db19b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+----------+------+\n|firstname|lastname|   id|  location|salary|\n+---------+--------+-----+----------+------+\n|    James|   Smith|36636|   NewYork|  3100|\n|  Michael|        |40288|California|  4300|\n|   Robert|Williams|42114|   Florida|  1400|\n|    Maria|   Jones|39192|   Florida|  5500|\n|      Jen|   Brown|34561|   NewYork|  3000|\n+---------+--------+-----+----------+------+\n\n+---------+----------+-----+----------+------+\n|firstname|middlename|   id|  location|salary|\n+---------+----------+-----+----------+------+\n|    James|          |36636|   NewYork|  3100|\n|  Michael|      Rose|40288|California|  4300|\n|   Robert|          |42114|   Florida|  1400|\n|    Maria|      Anne|39192|   Florida|  5500|\n|      Jen|      Mary|34561|   NewYork|  3000|\n+---------+----------+-----+----------+------+\n\n+---------+----------+--------+-----+------+\n|firstname|middlename|lastname|   id|salary|\n+---------+----------+--------+-----+------+\n|    James|          |   Smith|36636|  3100|\n|  Michael|      Rose|        |40288|  4300|\n|   Robert|          |Williams|42114|  1400|\n|    Maria|      Anne|   Jones|39192|  5500|\n|      Jen|      Mary|   Brown|34561|  3000|\n+---------+----------+--------+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Method 1: Directly using the name of the column\n",
    "df.drop(\"middlename\").show()\n",
    "\n",
    "# Method 2: Accessing the column name using . operator\n",
    "df.drop(df.lastname).show()\n",
    "\n",
    "# Method 3: Accessing the column using col function\n",
    "df.drop(col(\"location\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b63eaaa2-ba9c-41c3-a39c-9d9fbc173cbd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Dropping multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26eac8fc-ea4b-41e5-94b5-a8a5181ec3ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------+------+\n|firstname|lastname|  location|salary|\n+---------+--------+----------+------+\n|    James|   Smith|   NewYork|  3100|\n|  Michael|        |California|  4300|\n|   Robert|Williams|   Florida|  1400|\n|    Maria|   Jones|   Florida|  5500|\n|      Jen|   Brown|   NewYork|  3000|\n+---------+--------+----------+------+\n\n+---------+----------+------+\n|firstname|  location|salary|\n+---------+----------+------+\n|    James|   NewYork|  3100|\n|  Michael|California|  4300|\n|   Robert|   Florida|  1400|\n|    Maria|   Florida|  5500|\n|      Jen|   NewYork|  3000|\n+---------+----------+------+\n\n+---------+--------+------+\n|firstname|lastname|salary|\n+---------+--------+------+\n|    James|   Smith|  3100|\n|  Michael|        |  4300|\n|   Robert|Williams|  1400|\n|    Maria|   Jones|  5500|\n|      Jen|   Brown|  3000|\n+---------+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Directly using the name of the column\n",
    "df.drop(\"middlename\",\"id\").show()\n",
    "\n",
    "# Method 2: Accessing the column name using . operator\n",
    "df.drop(df.lastname,df.middlename,df.id).show()\n",
    "\n",
    "# Method 3: Using tuple\n",
    "cols=(\"middlename\",\"id\",\"location\")\n",
    "df.drop(*cols).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark Drop",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
