{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a76f4b23-d7dc-4bb1-9624-7aba39177e6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab350fec-9e7a-48e3-8bb9-3fb92c6f7f47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"Aggregate Functions\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba061556-fac0-4134-b121-d7d2c968f057",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "  ]\n",
    "schema = [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca97ecb6-7e71-4a38-85af-b917c666952a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Aggregate Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0defee51-ba1b-4e4e-95dc-9ed6dd6febdb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### approx_count_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a7fbd9a-6440-48a2-8ef6-b1ad05f26118",
     "showTitle": true,
     "title": " "
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approx_count_distinct: 6\n"
     ]
    }
   ],
   "source": [
    "# This function returns the count of distinct values from  column\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "print(\"approx_count_distinct: \"+\\\n",
    "    str(df.select(approx_count_distinct(\"salary\"))\n",
    "        .collect()[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "096e6089-9b06-4147-afe4-8abc400a2892",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f78c970b-2d40-475d-ad20-407ceeb6036c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg of column:3400.0\n"
     ]
    }
   ],
   "source": [
    "# This function returns the avg of the values from the given column\n",
    "from pyspark.sql.functions import avg\n",
    "print(\"avg of column:\"+\\\n",
    "    str(df.select(avg(\"salary\")).collect()[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71e8fa59-3bb9-4b70-abfd-71579841c139",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### collect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00be5f49-43df-4fd8-b798-d1d4ab8a4e41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collected list of values : ['James', 'Michael', 'Robert', 'Maria', 'James', 'Scott', 'Jen', 'Jeff', 'Kumar', 'Saif']\n"
     ]
    }
   ],
   "source": [
    "# This function returns the list of the non distinct values collected from the column \n",
    "from pyspark.sql.functions import collect_list\n",
    "print(\"collected list of values : \" + \\\n",
    "     str(df.select(collect_list(\"employee_name\")).collect()[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5440b575-1395-4029-9473-5d27ef467ede",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------+\n|names                                                                |\n+---------------------------------------------------------------------+\n|[James, Michael, Robert, Maria, James, Scott, Jen, Jeff, Kumar, Saif]|\n+---------------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(collect_list(\"employee_name\").alias(\"names\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d06af19d-0393-4756-a300-0c920e5eafab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### collect_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "972140d4-408f-471d-8551-9858c244b976",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collect set : [4600, 3000, 3900, 4100, 3300, 2000]\n"
     ]
    }
   ],
   "source": [
    "# This function returns the list of the distinct values collected from the column \n",
    "from pyspark.sql.functions import collect_set\n",
    "print(\"collect set : \" + \\\n",
    "     str(df.select(collect_set(\"salary\")) \\\n",
    "        .collect()[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b15ac0e2-6817-45c4-929d-07c14b416e21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n|collect_set(salary)                 |\n+------------------------------------+\n|[4600, 3000, 3900, 4100, 3300, 2000]|\n+------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "s=df.select(collect_set(\"salary\"))\n",
    "s.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a013a8e-057b-4fa5-88cf-a8ad7c1a1251",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f07774b-8dc8-49f6-b04e-c33abe29f19c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n|count(DISTINCT salary)|\n+----------------------+\n|                     6|\n+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "df2=df.select(countDistinct(\"salary\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5689852-b175-4a08-9901-1c0aa3032e40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n|count(DISTINCT department, salary)|\n+----------------------------------+\n|                                 8|\n+----------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df2=df.select(countDistinct(\"department\",\"salary\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "597efc46-eff7-4c4e-b347-acd132835c0d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7029139-e957-4186-b604-f84b53fdfc9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n|count(salary)|\n+-------------+\n|           10|\n+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "df2=df.select(count(\"salary\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f09c28c1-bf68-4cae-8ba1-4004524033f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc82461b-5546-4eac-ac07-68c7524e02e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n|first(salary)|\n+-------------+\n|         3000|\n+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first\n",
    "df.select(first(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc7bb041-dce4-445e-a83f-05c212b0a25e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "848979f3-a1ad-4704-b6f8-a4caa145a2e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n|last(salary)|\n+------------+\n|        4100|\n+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import last\n",
    "df.select(last(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f82b0c75-cc12-485b-8324-51705db3bc8c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ed6e9aa-89ad-48c0-adf6-5d38987c6a78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|max(salary)|\n+-----------+\n|       4600|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "df.select(max(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84327a23-58de-46c2-8bac-67906e156c39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2f27b68-b40c-4181-912c-ea04141ec029",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|min(salary)|\n+-----------+\n|       2000|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min\n",
    "df.select(min(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa3f9b94-39df-454c-ad8a-936b2c3fa760",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f67c5bd2-227a-49bd-aaf7-d2cb0f6bb5cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|avg(salary)|\n+-----------+\n|     3400.0|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "df.select(mean(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a17bc5d-af6c-479b-b2a6-b7f7178f9875",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### kurthosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6927169-7bbf-484f-985b-5e42fd33c86e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n|   kurtosis(salary)|\n+-------------------+\n|-0.6467803030303032|\n+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import kurtosis\n",
    "df.select(kurtosis(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d77c8a8c-582a-4151-8728-17f41f8e4ed6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37556782-c2c8-4d3d-8f0c-1e200589f3d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|    skewness(salary)|\n+--------------------+\n|-0.12041791181069571|\n+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness\n",
    "df.select(skewness(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43b5facd-5fc1-49d5-8ff7-0b02d295a189",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1d454b4-119c-43b2-acc9-6f6458ecd512",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n|stddev_samp(salary)|\n+-------------------+\n|  765.9416862050705|\n+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import stddev\n",
    "df.select(stddev(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b7c848d-f693-4300-be16-f582d430630f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### stddev_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8293dff-f89b-4b47-8b00-c890f96b0e2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n|stddev_samp(salary)|\n+-------------------+\n|  765.9416862050705|\n+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import stddev_samp\n",
    "df.select(stddev_samp(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78ddace6-ce82-471c-8a4e-b958247d5077",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### stddev_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f64b22ce-52b5-43df-9e88-6937e713b603",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n|stddev_pop(salary)|\n+------------------+\n|  726.636084983398|\n+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import stddev_pop\n",
    "df.select(stddev_pop(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "043725ee-9d4c-41bc-9516-34cb2a7b3589",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16eee463-e4ad-4513-93d7-410ad4b0c28d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|sum(salary)|\n+-----------+\n|      34000|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "df.select(sum(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1f7eea6-d418-4c9a-b47c-5ee65689c1fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### sum_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec143bf4-43d2-453f-881b-265f02c51e11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|sum(DISTINCT salary)|\n+--------------------+\n|               20900|\n+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum_distinct\n",
    "df.select(sum_distinct(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "accd8a51-3686-4468-abd4-dae8530dda35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "453e4c13-beed-49f5-aa3b-ec6eb9b65eb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n| var_samp(salary)|\n+-----------------+\n|586666.6666666666|\n+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import variance\n",
    "df.select(variance(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d41befc-5f28-4d2f-8b4f-1aa26bc75f00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Aggregate functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
