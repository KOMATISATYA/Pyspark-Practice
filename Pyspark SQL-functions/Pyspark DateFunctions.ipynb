{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cdd15be-2447-40dd-9a54-95c95c100363",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b202dda-4d84-4dd8-b04e-faa7dc0e5ed8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"Date Function\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc8ad0ad-d6ed-486c-8d5c-f9364edc7cc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: string (nullable = true)\n |-- input: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "data=[[\"1\",\"2020-02-01\"],[\"2\",\"2019-03-01\"],[\"3\",\"2021-04-05\"]]\n",
    "df=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "829fe271-e025-41bf-a996-218ee2f5286a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Date Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf845426-133c-46bd-bdef-1af8e1af3bfd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### current_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe5172d3-cb92-4166-9bde-3915d97cb738",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|currentDate|\n+-----------+\n| 2024-03-30|\n| 2024-03-30|\n| 2024-03-30|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#current date returns the current date\n",
    "from pyspark.sql.functions import current_date\n",
    "df.select(current_date().alias(\"currentDate\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fa05eec-e036-4f7c-8c42-9bdb8433ae30",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### to_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b8c4999-6f38-49c3-961c-aa685fda1430",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n|     input|to_date|\n+----------+-------+\n|2020-02-01|   null|\n|2019-03-01|   null|\n|2021-04-05|   null|\n+----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# to_date converts the string type to date type\n",
    "from pyspark.sql.functions import to_date,col\n",
    "df1=df.select(col(\"input\"),\n",
    "              to_date(col(\"input\"),\"dd-MM-yyyy\").alias(\"to_date\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "193111cc-ce82-4dc5-b1a1-f2fed831ad5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- input: string (nullable = true)\n |-- to_date: date (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a352f072-f8c2-45ec-8b4e-24dc2f85b879",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: string (nullable = true)\n |-- input: string (nullable = true)\n |-- new_input: date (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"new_input\",to_date(\"input\")).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "880146fd-ab06-428f-a85d-ef66bd5f1b3f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### dayofweek()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3efefb03-04ca-464e-9f9e-376b38b22c7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- input: string (nullable = true)\n |-- Day_of_week: integer (nullable = true)\n\n+----------+-----------+\n|     input|Day_of_week|\n+----------+-----------+\n|2020-02-01|          7|\n|2019-03-01|          6|\n|2021-04-05|          2|\n+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dayofweek\n",
    "df2=df.select(col(\"input\"),dayofweek(\"input\").alias(\"Day_of_week\"))\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11f14d89-a88a-44e4-a0cf-9ce12613c5b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### dayofmonth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3298019c-a608-45eb-a1d0-46706c3f228d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n|     input|Day_of_Month|\n+----------+------------+\n|2020-02-01|           1|\n|2019-03-01|           1|\n|2021-04-05|           5|\n+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dayofmonth\n",
    "df.select(col(\"input\"),dayofmonth(\"input\").alias(\"Day_of_Month\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8bbced8-250c-4f72-b5f2-7d32261009e3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22d282c7-bf5f-42ad-b165-e4007cdafc92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+\n| id|     input|Day_Of_Year|\n+---+----------+-----------+\n|  1|2020-02-01|         32|\n|  2|2019-03-01|         60|\n|  3|2021-04-05|         95|\n+---+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dayofyear\n",
    "df.withColumn(\"Day_Of_Year\",dayofyear(\"input\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e636ca47-a09a-408c-95a2-5a31fbf985c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### weekofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7252c366-94d8-45fc-a852-7ade18f435eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+\n| id|     input|Week_of_year|\n+---+----------+------------+\n|  1|2020-02-01|           5|\n|  2|2019-03-01|           9|\n|  3|2021-04-05|          14|\n+---+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import weekofyear\n",
    "df.withColumn(\"Week_of_year\",weekofyear(\"input\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee84ba32-06f1-4f00-a365-39b37d4a0ca7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### yer,month,quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46fc5c52-9d17-489c-b06b-2a9e14b870ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----+-----+-------+\n| id|     input|Year|Month|Quarter|\n+---+----------+----+-----+-------+\n|  1|2020-02-01|2020|    2|      1|\n|  2|2019-03-01|2019|    3|      1|\n|  3|2021-04-05|2021|    4|      2|\n+---+----------+----+-----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# quarter returns the quarter number(upto 3 months - 1st quarter, 4 to 6 months - 2nd quarter, ...)\n",
    "from pyspark.sql.functions import year, month, quarter\n",
    "df.withColumn(\"Year\", year(\"input\"))\\\n",
    "    .withColumn(\"Month\", month(\"input\"))\\\n",
    "    .withColumn(\"Quarter\", quarter(\"input\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72c52e4c-3c07-48e0-add1-bdb483d2f2de",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### last_day()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ae7c51c-08be-4fd7-92c6-4464124dc789",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+\n| id|     input|   lastDay|\n+---+----------+----------+\n|  1|2020-02-01|2020-02-29|\n|  2|2019-03-01|2019-03-31|\n|  3|2021-04-05|2021-04-30|\n+---+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import last_day\n",
    "df.withColumn(\"lastDay\",last_day(\"input\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "224c34ba-f642-4865-a8fd-9ab3fbe8f4c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### next_day()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdca3e55-3c90-4fd2-a14e-5d3ef11542ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+\n| id|     input|   nextDay|\n+---+----------+----------+\n|  1|2020-02-01|2020-02-03|\n|  2|2019-03-01|2019-03-04|\n|  3|2021-04-05|2021-04-12|\n+---+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import next_day\n",
    "df.withColumn(\"nextDay\",next_day(\"input\",\"Monday\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0afe94fc-e0be-4d23-aefc-945438e49c77",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### add_months,date_add,date_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4de4d695-ab16-4f55-8e32-e3fbf239dd4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+---------------+----------+----------+\n| id|     input| add_month|add_month_with-|  date_add|  date_sub|\n+---+----------+----------+---------------+----------+----------+\n|  1|2020-02-01|2020-05-01|     2019-11-01|2020-02-05|2020-01-30|\n|  2|2019-03-01|2019-06-01|     2018-12-01|2019-03-05|2019-02-27|\n|  3|2021-04-05|2021-07-05|     2021-01-05|2021-04-09|2021-04-03|\n+---+----------+----------+---------------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import add_months, date_add, date_sub\n",
    "df.withColumn(\"add_month\", add_months(\"input\", 3))\\\n",
    "    .withColumn(\"add_month_with-\", add_months(\"input\", -3))\\\n",
    "    .withColumn(\"date_add\", date_add(\"input\", 4)) \\\n",
    "    .withColumn(\"date_sub\", date_sub(\"input\", 2)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "498cb8db-e34f-4e47-9523-31fdc5be9910",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### datediff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0315c654-a4bc-4264-8b6e-aef1dbff0ad4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n|     input|date_difference|\n+----------+---------------+\n|2020-02-01|           1519|\n|2019-03-01|           1856|\n|2021-04-05|           1090|\n+----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff\n",
    "df.select(col(\"input\"),\\\n",
    "    datediff(current_date(),col(\"input\")).alias(\"date_difference\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51958724-c84e-4d4c-8c7b-026e78ddda9a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### months_between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8517dfd-4760-4117-aff0-c3edc665ed4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------------+\n| id|     input|    months_between|\n+---+----------+------------------+\n|  1|2020-02-01|49.935483870967744|\n|  2|2019-03-01|60.935483870967744|\n|  3|2021-04-05|35.806451612903224|\n+---+----------+------------------+\n\n+---+----------+--------------+\n| id|     input|months_between|\n+---+----------+--------------+\n|  1|2020-02-01|   49.93548387|\n|  2|2019-03-01|   60.93548387|\n|  3|2021-04-05|   35.80645161|\n+---+----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import months_between\n",
    "df.withColumn(\"months_between\", months_between(current_date(), col(\"input\"), False)).show()\n",
    "df.withColumn(\"months_between\", months_between(current_date(), col(\"input\"), True)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a82181b5-f90e-413e-bd14-5d945acfc522",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28259753-2851-47f8-82b6-64de0a3ada53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------+--------------+-----------+\n| id|     input|truncatedYear|truncatedMonth|trunctedDay|\n+---+----------+-------------+--------------+-----------+\n|  1|2020-02-01|   2020-01-01|    2020-02-01| 2020-01-27|\n|  2|2019-03-01|   2019-01-01|    2019-03-01| 2019-02-25|\n|  3|2021-04-05|   2021-01-01|    2021-04-01| 2021-04-05|\n+---+----------+-------------+--------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import trunc\n",
    "df.withColumn(\"truncatedYear\",trunc(\"input\",\"Year\"))\\\n",
    "    .withColumn(\"truncatedMonth\",trunc(\"input\",\"Month\"))\\\n",
    "        .withColumn(\"trunctedDay\",trunc(\"input\",\"Week\"))\\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97c867b4-3026-4867-bf33-bf783096879f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### date_trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d31769bc-ee29-4ef5-83c9-a6be87df8847",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+-------------------+-------------------+-------------------+\n| id|     input|      truncatedYear|     truncatedMonth|       truncatedDay|    truncatedQurter|\n+---+----------+-------------------+-------------------+-------------------+-------------------+\n|  1|2020-02-01|2020-01-01 00:00:00|2020-02-01 00:00:00|2020-02-01 00:00:00|2020-01-01 00:00:00|\n|  2|2019-03-01|2019-01-01 00:00:00|2019-03-01 00:00:00|2019-03-01 00:00:00|2019-01-01 00:00:00|\n|  3|2021-04-05|2021-01-01 00:00:00|2021-04-01 00:00:00|2021-04-05 00:00:00|2021-04-01 00:00:00|\n+---+----------+-------------------+-------------------+-------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_trunc\n",
    "df.withColumn(\"truncatedYear\",date_trunc(\"Year\",\"input\"))\\\n",
    "    .withColumn(\"truncatedMonth\",date_trunc(\"Month\",\"input\"))\\\n",
    "        .withColumn(\"truncatedDay\",date_trunc(\"Day\",\"input\"))\\\n",
    "            .withColumn(\"truncatedQurter\",date_trunc(\"Quarter\",\"input\"))\\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b80d723-4225-45d3-937e-b9077e6d9d1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+-------------------+-------------------+-------------------+\n| id|     input|      truncatedWeek|      truncatedHour|    truncatedMinute|    truncatedSecond|\n+---+----------+-------------------+-------------------+-------------------+-------------------+\n|  1|2020-02-01|2020-01-27 00:00:00|2020-02-01 00:00:00|2020-02-01 00:00:00|2020-02-01 00:00:00|\n|  2|2019-03-01|2019-02-25 00:00:00|2019-03-01 00:00:00|2019-03-01 00:00:00|2019-03-01 00:00:00|\n|  3|2021-04-05|2021-04-05 00:00:00|2021-04-05 00:00:00|2021-04-05 00:00:00|2021-04-05 00:00:00|\n+---+----------+-------------------+-------------------+-------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "## Remaining Most possible values : ‘second’, ‘minute’, ‘hour’, ‘week’\n",
    "from pyspark.sql.functions import date_trunc\n",
    "df.withColumn(\"truncatedWeek\", date_trunc(\"week\", \"input\"))\\\n",
    "    .withColumn(\"truncatedHour\", date_trunc(\"hour\", \"input\"))\\\n",
    "    .withColumn(\"truncatedMinute\", date_trunc(\"minute\", \"input\"))\\\n",
    "    .withColumn(\"truncatedSecond\", date_trunc(\"second\", \"input\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71707976-2965-4640-ae7d-966dda3ff8b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### form_unixtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b78023ac-ba0a-493a-a57f-89fe3e3e0ffa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+\n|unix_timestamp|          timestamp|\n+--------------+-------------------+\n|    1612345678|2021-02-03 09:47:58|\n|    1623456789|2021-06-12 00:13:09|\n|    1634567890|2021-10-18 14:38:10|\n+--------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_unixtime\n",
    "\n",
    "data = [(1612345678,),\n",
    "        (1623456789,),\n",
    "        (1634567890,)]\n",
    "columns = [\"unix_timestamp\"]\n",
    "df2 = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Using from_unixtime to convert Unix timestamps to timestamps\n",
    "df2=df2.withColumn(\"timestamp\",from_unixtime(\"unix_timestamp\"))\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52b3f6a0-fd86-4cdf-880a-f8977a4ec5d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### unix_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c5419fd-ea36-463a-a758-b3bae3d3aae2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+\n| id|     input|timestamp|\n+---+----------+---------+\n|  1|2020-02-01|     null|\n|  2|2019-03-01|     null|\n|  3|2021-04-05|     null|\n+---+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp\n",
    "df.withColumn('timestamp',unix_timestamp(\"input\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8035423-c439-4112-beb3-3b41ea979962",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Time Stamp Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d91c60c8-4fb5-4da4-be0c-de3e017603eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data=[[\"1\",\"2020-02-01 11:01:19.06\"],[\"2\",\"2019-03-01 12:01:19.406\"],[\"3\",\"2021-03-01 12:01:19.406\"]]\n",
    "df3=spark.createDataFrame(data,[\"id\",\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2b91a1b-9477-4e5d-9dda-21e07dcda90b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data=[[\"1\",\"02-01-2020 11 01 19 06\"],[\"2\",\"03-01-2019 12 01 19 406\"],[\"3\",\"03-01-2021 12 01 19 406\"]]\n",
    "df2=spark.createDataFrame(data,[\"id\",\"input\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1da79242-4693-4310-b3bc-b49ec4181844",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11f67187-7866-429b-83c0-6a4a57156285",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+-----------------------+\n|id |input                  |Current_Time           |\n+---+-----------------------+-----------------------+\n|1  |02-01-2020 11 01 19 06 |2024-03-30 15:28:58.862|\n|2  |03-01-2019 12 01 19 406|2024-03-30 15:28:58.862|\n|3  |03-01-2021 12 01 19 406|2024-03-30 15:28:58.862|\n+---+-----------------------+-----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "df2.withColumn(\"Current_Time\", current_timestamp()).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b42db651-d58a-49a0-8c6a-6073741102ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1c90cd0-3a1f-44b1-9f57-8a3fc344df88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+-----------------------+\n|id |input                  |Coss_TS                |\n+---+-----------------------+-----------------------+\n|1  |02-01-2020 11 01 19 06 |2020-01-02 11:01:19.06 |\n|2  |03-01-2019 12 01 19 406|2019-01-03 12:01:19.406|\n|3  |03-01-2021 12 01 19 406|2021-01-03 12:01:19.406|\n+---+-----------------------+-----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "df2.withColumn(\"Coss_TS\",to_timestamp(\"input\", \"dd-MM-yyyy HH mm ss SSS\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af481fa4-e547-4950-a5e1-a833df65c826",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9ad2e94-7d29-447e-b023-1aea1387c22f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----+\n| id|               input|hour|\n+---+--------------------+----+\n|  1|2020-02-01 11:01:...|  11|\n|  2|2019-03-01 12:01:...|  12|\n|  3|2021-03-01 12:01:...|  12|\n+---+--------------------+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour\n",
    "df3.withColumn(\"hour\",hour(\"input\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffef658d-fa13-4a33-a4b5-ebe6a826a0b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5603057-d166-4bbf-8d6f-5c3b5313ccca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+---+\n|id |input                  |min|\n+---+-----------------------+---+\n|1  |2020-02-01 11:01:19.06 |1  |\n|2  |2019-03-01 12:01:19.406|1  |\n|3  |2021-03-01 12:01:19.406|1  |\n+---+-----------------------+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import minute\n",
    "df3.withColumn(\"min\",minute(\"input\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9971e045-4124-4998-8dcd-b8c9ea090d9a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94e23cdc-aadc-43d3-a0be-88a6c9a58dbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+---+\n|id |input                  |sec|\n+---+-----------------------+---+\n|1  |2020-02-01 11:01:19.06 |19 |\n|2  |2019-03-01 12:01:19.406|19 |\n|3  |2021-03-01 12:01:19.406|19 |\n+---+-----------------------+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import second\n",
    "df3.withColumn(\"sec\",second(\"input\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cde680d3-2b04-4d6c-8fc7-8e2cf9b14704",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Date Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb1a27ea-685c-41d5-8d56-e416c70c09fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"2022-01-15 08:30:45\",),\n",
    "        (\"2022-02-20 12:15:30\",),\n",
    "        (\"2022-03-25 18:45:15\",)]\n",
    "columns = [\"timestamp\"]\n",
    "df4 = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Convert the string timestamp to a timestamp type\n",
    "df4 = df4.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20f2d74e-cd5e-4e86-8b03-642b3df674f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n|          timestamp|new_format|\n+-------------------+----------+\n|2022-01-15 08:30:45|15-01-2022|\n|2022-02-20 12:15:30|20-02-2022|\n|2022-03-25 18:45:15|25-03-2022|\n+-------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "df4.withColumn(\"new_format\", date_format(\"timestamp\", \"dd-MM-yyyy\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d201e14f-2f43-4e86-ae86-89913e03caf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+\n|          timestamp|year|\n+-------------------+----+\n|2022-01-15 08:30:45|2022|\n|2022-02-20 12:15:30|2022|\n|2022-03-25 18:45:15|2022|\n+-------------------+----+\n\n"
     ]
    }
   ],
   "source": [
    "df4.withColumn(\"year\", date_format(\"timestamp\", \"yyyy\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05c82f27-5ab8-4009-bf71-056024cb76e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n|          timestamp|month|\n+-------------------+-----+\n|2022-01-15 08:30:45|   01|\n|2022-02-20 12:15:30|   02|\n|2022-03-25 18:45:15|   03|\n+-------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df4.withColumn(\"month\",date_format(\"timestamp\",\"MM\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97014927-f4d0-4a00-9276-3e28e9a97110",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n|          timestamp|short_month|\n+-------------------+-----------+\n|2022-01-15 08:30:45|        Jan|\n|2022-02-20 12:15:30|        Feb|\n|2022-03-25 18:45:15|        Mar|\n+-------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df4.withColumn(\"short_month\",date_format(\"timestamp\",\"MMM\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55350fdc-500c-4cf1-99b0-fa4d6d15bc1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n|          timestamp|full_month|\n+-------------------+----------+\n|2022-01-15 08:30:45|   January|\n|2022-02-20 12:15:30|  February|\n|2022-03-25 18:45:15|     March|\n+-------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df4.withColumn(\"full_month\",date_format(\"timestamp\",\"MMMM\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42207ffe-4cec-4662-8525-a2305a6424cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n|          timestamp|am_pm|\n+-------------------+-----+\n|2022-01-15 08:30:45|   AM|\n|2022-02-20 12:15:30|   PM|\n|2022-03-25 18:45:15|   PM|\n+-------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df4.withColumn(\"am_pm\",date_format(\"timestamp\",'a')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d2ff47b-bda0-486e-bfcb-a468e82b6b10",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Timestamp Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb96cf76-83e6-41c6-8f9c-4f12a9cffcf9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------------+\n|timestamp          |am_pm                 |\n+-------------------+----------------------+\n|2022-01-15 08:30:45|2022-01-15 08:30:45 AM|\n|2022-02-20 12:15:30|2022-02-20 12:15:30 PM|\n|2022-03-25 18:45:15|2022-03-25 06:45:15 PM|\n+-------------------+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df4 = df4.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
    "\n",
    "# a in the format explains it if it is AM or PM by converting the 24h into 12h\n",
    "df4 = df4.withColumn(\"am_pm\", date_format(\"timestamp\", \"yyyy-MM-dd hh:mm:ss a\"))\n",
    "df4.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d5b3f7d-3c5a-45b9-8a4c-ddd7a113bd5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-------------------+\n|          timestamp|               am_pm|     formatted_date|\n+-------------------+--------------------+-------------------+\n|2022-01-15 08:30:45|2022-01-15 08:30:...|15-01-2022 08:30:45|\n|2022-02-20 12:15:30|2022-02-20 12:15:...|20-02-2022 12:15:30|\n|2022-03-25 18:45:15|2022-03-25 06:45:...|25-03-2022 18:45:15|\n+-------------------+--------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df4.withColumn(\"formatted_date\",date_format(\"timestamp\",\"dd-MM-yyyy HH:mm:ss\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "635e18c3-68ba-45c9-821c-2cca49c38eec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+\n|          timestamp|               am_pm|        abbrev_month|\n+-------------------+--------------------+--------------------+\n|2022-01-15 08:30:45|2022-01-15 08:30:...|15/Jan/2022 08:30:45|\n|2022-02-20 12:15:30|2022-02-20 12:15:...|20/Feb/2022 12:15:30|\n|2022-03-25 18:45:15|2022-03-25 06:45:...|25/Mar/2022 18:45:15|\n+-------------------+--------------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df4.withColumn(\"abbrev_month\",date_format(\"timestamp\",\"dd/MMM/yyyy HH:mm:ss\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04bb087b-9e3b-42a4-a731-4635c23e4ca2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+\n|          timestamp|               am_pm|          month_year|\n+-------------------+--------------------+--------------------+\n|2022-01-15 08:30:45|2022-01-15 08:30:...|Jan-2022 08:30:45 AM|\n|2022-02-20 12:15:30|2022-02-20 12:15:...|Feb-2022 12:15:30 PM|\n|2022-03-25 18:45:15|2022-03-25 06:45:...|Mar-2022 18:45:15 PM|\n+-------------------+--------------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df4.withColumn(\"month_year\",date_format(\"timestamp\",\"MMM-yyyy HH:mm:ss a\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4a337a5-1dd4-4d12-8e1d-e6be4f36a23b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----------+\n|          timestamp|               am_pm|day_of_week|\n+-------------------+--------------------+-----------+\n|2022-01-15 08:30:45|2022-01-15 08:30:...|   Saturday|\n|2022-02-20 12:15:30|2022-02-20 12:15:...|     Sunday|\n|2022-03-25 18:45:15|2022-03-25 06:45:...|     Friday|\n+-------------------+--------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df4.withColumn(\"day_of_week\",date_format(\"timestamp\",\"EEEE\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "289490ee-3a5c-43f3-888f-b73ab7ab186b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----------+\n|          timestamp|               am_pm|day_of_week|\n+-------------------+--------------------+-----------+\n|2022-01-15 08:30:45|2022-01-15 08:30:...|        Sat|\n|2022-02-20 12:15:30|2022-02-20 12:15:...|        Sun|\n|2022-03-25 18:45:15|2022-03-25 06:45:...|        Fri|\n+-------------------+--------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df4.withColumn(\"day_of_week\",date_format(\"timestamp\",\"E\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b003f853-0bb7-488c-ab1c-9275cc269f97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark DateFunctions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
